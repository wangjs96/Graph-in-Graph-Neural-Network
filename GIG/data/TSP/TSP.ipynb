{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "from scipy import sparse as sp\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../') # go to root folder of the project\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.superpixels import SuperPixDatasetDGL \n",
    "\n",
    "from data.data import LoadData\n",
    "from torch.utils.data import DataLoader\n",
    "from data.superpixels import SuperPixDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'TSP'\n",
    "dataset = LoadData(DATASET_NAME)\n",
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from bisect import bisect_right\n",
    "\n",
    "\n",
    "class MLPReadout(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, L=3): #L=nb_hidden_layers\n",
    "        super().__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim//2**l , input_dim//2**(l+1) , bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim//2**L , output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = F.relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedGCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, batch_norm, residual=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_dim\n",
    "        self.out_channels = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        if input_dim != output_dim:\n",
    "            self.residual = False\n",
    "        \n",
    "        self.A = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.B = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.C = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.D = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.E = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "    \n",
    "    \n",
    "    def act(self, x):\n",
    "        return x*(torch.tanh(F.softplus(x)))\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        Bh_j = edges.src['Bh']    \n",
    "        e_ij = edges.data['Ce'] +  edges.src['Dh'] + edges.dst['Eh'] # e_ij = Ce_ij + Dhi + Ehj\n",
    "        edges.data['e'] = e_ij\n",
    "        return {'Bh_j' : Bh_j, 'e_ij' : e_ij}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        Ah_i = nodes.data['Ah']\n",
    "        Bh_j = nodes.mailbox['Bh_j']\n",
    "        e = nodes.mailbox['e_ij'] \n",
    "        sigma_ij = torch.sigmoid(e) # sigma_ij = sigmoid(e_ij)\n",
    "        h = Ah_i + torch.sum( sigma_ij * Bh_j, dim=1 ) / ( torch.sum( sigma_ij, dim=1 ) + 1e-6 )  # hi = Ahi + sum_j eta_ij/sum_j' eta_ij' * Bhj <= dense attention       \n",
    "        return {'h' : h}\n",
    "    \n",
    "    def forward(self, g, h, e):\n",
    "        h_in = h # for residual connection\n",
    "        e_in = e # for residual connection\n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) \n",
    "        g.ndata['Bh'] = self.B(h) \n",
    "        g.ndata['Dh'] = self.D(h)\n",
    "        g.ndata['Eh'] = self.E(h) \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e) \n",
    "        \n",
    "        g.apply_edges(fn.u_add_v('Dh', 'Eh', 'DEh'))\n",
    "        g.edata['e'] = g.edata['DEh'] + g.edata['Ce']\n",
    "        g.edata['sigma'] = torch.sigmoid(g.edata['e'])\n",
    "        g.update_all(fn.u_mul_e('Bh', 'sigma', 'm'), fn.sum('m', 'sum_sigma_h'))\n",
    "        g.update_all(fn.copy_e('sigma', 'm'), fn.sum('m', 'sum_sigma'))\n",
    "        g.ndata['h'] = g.ndata['Ah'] + g.ndata['sum_sigma_h'] / (g.ndata['sum_sigma'] + 1e-6)\n",
    "\n",
    "        h = g.ndata['h'] # result of graph convolution\n",
    "        e = g.edata['e'] # result of graph convolution\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            h = self.bn_node_h(h) # batch normalization  \n",
    "            e = self.bn_node_e(e) # batch normalization  \n",
    "            \n",
    "        h = self.act(h) # non-linear activation\n",
    "        e = self.act(e) # non-linear activation\n",
    "#         h = F.leaky_relu(h) # non-linear activation\n",
    "#         e = F.leaky_relu(e) # non-linear activation\n",
    "        \n",
    "        if self.residual:\n",
    "            h = h_in + h # residual connection\n",
    "            e = e_in + e # residual connection\n",
    "        \n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        e = F.dropout(e, self.dropout, training=self.training)\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}(in_channels={}, out_channels={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels)\n",
    "\n",
    "    \n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, net_params, update_type):\n",
    "        super().__init__()\n",
    "        in_dim_node = net_params['in_dim']\n",
    "        in_dim_edge = net_params['in_dim_edge']\n",
    "        hidden_dim = net_params['hidden_dim']\n",
    "        out_dim = net_params['out_dim']\n",
    "        n_classes = net_params['n_classes']\n",
    "        dropout = net_params['dropout']\n",
    "        n_layers = net_params['L']\n",
    "        self.readout = net_params['readout']\n",
    "        self.batch_norm = net_params['batch_norm']\n",
    "        self.residual = net_params['residual']\n",
    "        self.edge_feat = net_params['edge_feat']\n",
    "        self.device = net_params['device']\n",
    "        self.update_type = update_type\n",
    "\n",
    "        if self.update_type == 1:\n",
    "            self.embedding_h1 = nn.Linear(in_dim_node, hidden_dim)\n",
    "            self.embedding_e1 = nn.Linear(in_dim_edge, hidden_dim)\n",
    "        else:\n",
    "            self.embedding_h2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.embedding_e2 = nn.Linear(in_dim_edge, hidden_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n",
    "                                                    self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n",
    "        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.batch_norm, self.residual))\n",
    "\n",
    "    def forward(self, g, h, e, stage):\n",
    "        if stage == 0:\n",
    "            if self.update_type == 1:\n",
    "                h = self.embedding_h1(h)\n",
    "                e = self.embedding_e1(e)\n",
    "            else:\n",
    "                h = self.embedding_h2(h)\n",
    "                e = self.embedding_e2(e)\n",
    "        \n",
    "        for conv in self.layers:\n",
    "            h, e = conv(g, h, e)\n",
    "        return h, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGLayer(nn.Module):\n",
    "    def __init__(self, net_params, device, is_last_layer):\n",
    "        super(GIGLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.is_last_layer = is_last_layer\n",
    "        SGU_params = net_params.copy()\n",
    "        GGU_P1_params = net_params.copy()\n",
    "        GGU_P2_params = net_params.copy()\n",
    "        SGU_params['L'] = 12\n",
    "        GGU_P1_params['L'] = 1\n",
    "        GGU_P2_params['L'] = 1\n",
    "        GGU_P1_params['in_dim'] = net_params['hidden_dim']\n",
    "        GGU_P2_params['in_dim'] = net_params['hidden_dim']\n",
    "        self.SGU = GNN(SGU_params, 1)\n",
    "        self.GGU_Part1 = GNN(GGU_P1_params, 2)\n",
    "        self.GGU_Part2 = GNN(GGU_P2_params, 2)\n",
    "        self.MLP_layer = MLPReadout(net_params['out_dim']*2, net_params['n_classes'])\n",
    "    \n",
    "    def forward(self, SGU_graph, GGU_P1_graph, GGU_P2_graph, h, SGU_e, GGU_P1_e, GGU_P2_e, original_length, stage, g_original=None):\n",
    "        h1, e1 = self.SGU(SGU_graph, h, SGU_e, stage)\n",
    "        h2, e2 = self.GGU_Part1(GGU_P1_graph, h1, GGU_P1_e, stage)\n",
    "        h3, e3 = self.GGU_Part2(GGU_P2_graph, h2, GGU_P2_e, stage)\n",
    "\n",
    "        if not self.is_last_layer:\n",
    "            return SGU_graph, GGU_P1_graph, GGU_P2_graph, h3, e1, e2, e3\n",
    "        else:\n",
    "            g_original.ndata['h'] = h3[:original_length, :]\n",
    "            def _edge_feat(edges):\n",
    "                e = torch.cat([edges.src['h'], edges.dst['h']], dim=1)\n",
    "                e = self.MLP_layer(e)\n",
    "                return {'e': e}\n",
    "            g_original.apply_edges(_edge_feat)\n",
    "            return g_original.edata['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GIGNet(nn.Module):\n",
    "    def __init__(self, net_params, n_classes, n_layers, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList([ GIGLayer(net_params, device, False) for _ in range(n_layers-1) ])\n",
    "        self.layers.append(GIGLayer(net_params, device, True))\n",
    "        self.in_dim = net_params['in_dim']\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def proxy_edge_construction(self, proxy_vertex, sub_vertex, num_proxy_edges):\n",
    "        sub_vertex = sub_vertex.permute(1,0).to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', proxy_vertex, sub_vertex).to(self.device)\n",
    "        _, target_inds_sim = si.topk(k=num_proxy_edges, dim=1, largest=True)\n",
    "        _, target_inds_nonsim = si.topk(k=num_proxy_edges, dim=1, largest=False)\n",
    "        return target_inds_nonsim.squeeze(0).to(self.device), target_inds_sim.squeeze(0).to(self.device)\n",
    "\n",
    "    def global_neighbors(self, h, num_global_neighbors):\n",
    "        h = h.to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', h, h.transpose(0, 1)).to(self.device)\n",
    "        diag = torch.diag(si).to(self.device)\n",
    "        a_diag = torch.diag_embed(diag).to(self.device)\n",
    "        si = (si - a_diag).to(self.device)\n",
    "        _, target_inds_sim = si.topk(k=num_global_neighbors, dim=1, largest=True)\n",
    "        _, target_inds_nonsim = si.topk(k=num_global_neighbors, dim=1, largest=False)\n",
    "        return target_inds_sim.to(self.device), target_inds_nonsim.to(self.device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def GDG(self, graphs, indices, edge_lengths, node_lengths):\n",
    "        global_src = torch.zeros((0)).to(self.device)\n",
    "        global_tgt = torch.zeros((0)).to(self.device)\n",
    "        src1 = torch.zeros((0)).to(self.device)\n",
    "        tgt1 = torch.zeros((0)).to(self.device)\n",
    "        src3 = torch.zeros((0)).to(self.device)\n",
    "        tgt3 = torch.zeros((0)).to(self.device)\n",
    "        node_index = 0\n",
    "        current_ind = graphs.ndata['feat'].shape[0]\n",
    "        proxy_inds = []\n",
    "        proxy_vertices = torch.zeros((0)).to(self.device)\n",
    "        proxy_vertices_gi_lo = torch.zeros((0)).to(self.device)\n",
    "        proxy_edge_counter = 0\n",
    "        for ind, indice in enumerate(indices):\n",
    "            node_length = node_lengths[ind]\n",
    "            edge_length = edge_lengths[ind]\n",
    "            \n",
    "            h = graphs.ndata['feat'][node_index:node_index+node_length, :].to(self.device)\n",
    "            \n",
    "            proxy_vertex = h.mean(dim=0).reshape(1, self.in_dim).to(self.device)\n",
    "            proxy_vertex_li_go = nn.init.xavier_uniform_(proxy_vertex)\n",
    "            proxy_vertex_gi_lo = nn.init.xavier_uniform_(proxy_vertex)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex_li_go), dim=0)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex_gi_lo), dim=0)\n",
    "            proxy_vertices_gi_lo = torch.cat((proxy_vertices_gi_lo, proxy_vertex_gi_lo), dim=0)\n",
    "            \n",
    "            proxy_node_length = int(node_length * 0.1)\n",
    "            proxy_edge_counter += proxy_node_length\n",
    "            \n",
    "            \n",
    "            src1_sub, tgt3_sub = self.proxy_edge_construction(proxy_vertex, h, proxy_node_length)\n",
    "            src1_sub = src1_sub + node_index\n",
    "            tgt3_sub = tgt3_sub + node_index\n",
    "            tgt1_sub = torch.full([proxy_node_length,], current_ind).to(self.device)   \n",
    "            \n",
    "            src3_sub = torch.cat((torch.full([proxy_node_length,], current_ind+1).to(self.device), torch.tensor([current_ind]).to(self.device)))\n",
    "            tgt3_sub = torch.cat((tgt3_sub, torch.tensor([current_ind+1]).to(self.device)))\n",
    "            \n",
    "            \n",
    "            src1 = torch.cat((src1, src1_sub))\n",
    "            tgt1 = torch.cat((tgt1, tgt1_sub))\n",
    "            src3 = torch.cat((src3, src3_sub))\n",
    "            tgt3 = torch.cat((tgt3, tgt3_sub))\n",
    "            \n",
    "            \n",
    "            node_index += node_length\n",
    "            proxy_inds.append(current_ind+1)\n",
    "            current_ind += 2\n",
    "        \n",
    "        src1 = torch.cat((graphs.adj()._indices()[0].to(self.device), src1))\n",
    "        tgt1 = torch.cat((graphs.adj()._indices()[1].to(self.device), tgt1))\n",
    "        SGU_g = dgl.graph((src1.long(), tgt1.long())).to(self.device)\n",
    "        SGU_g.add_nodes(1)\n",
    "        GGU_P2_g = dgl.graph((src3.long(), tgt3.long())).to(self.device)\n",
    "        \n",
    "        h = torch.cat((graphs.ndata['feat'], proxy_vertices))\n",
    "        SGU_k = torch.cat((graphs.edata['feat'], torch.zeros(proxy_edge_counter, 1).to(self.device)))\n",
    "        SGU_k = nn.init.xavier_uniform_(SGU_k)\n",
    "        GGU_P2_k = torch.zeros(proxy_edge_counter+len(indices), 1).to(self.device)\n",
    "#         GGU_P2_k = torch.ones(graphs.num_nodes(), 1).to(self.device)\n",
    "        GGU_P2_k = nn.init.xavier_uniform_(GGU_P2_k)\n",
    "        \n",
    "        num_global_neighbors = 9\n",
    "        proxy_inds = torch.tensor(proxy_inds).to(self.device)        \n",
    "        num_global_neighbors = int(num_global_neighbors/2)\n",
    "        target_inds_similar, target_inds_nonsimilar = self.global_neighbors(proxy_vertices_gi_lo, num_global_neighbors)\n",
    " \n",
    "\n",
    "        for ind, i in enumerate(proxy_inds):\n",
    "            src1_1 = proxy_inds[target_inds_similar[ind,:]].to(self.device)\n",
    "            tgt1_1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            src1_2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt1_2 = proxy_inds[target_inds_similar[ind,:]].to(self.device)\n",
    "            \n",
    "            src2_1 = proxy_inds[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            tgt2_1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            src2_2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt2_2 = proxy_inds[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            \n",
    "            src1 = torch.cat((src1_1, src1_2))\n",
    "            tgt1 = torch.cat((tgt1_1, tgt1_2))\n",
    "            src2 = torch.cat((src2_1, src2_2))\n",
    "            tgt2 = torch.cat((tgt2_1, tgt2_2))\n",
    "            \n",
    "            src_ggu2 = torch.cat((src1, src2))\n",
    "            tgt_ggu2 = torch.cat((tgt1, tgt2))\n",
    "                        \n",
    "            global_src = torch.cat((global_src, src_ggu2))\n",
    "            global_tgt = torch.cat((global_tgt, tgt_ggu2)) \n",
    "\n",
    "        \n",
    "\n",
    "        GGU_P1_g = dgl.graph((global_src.long(), global_tgt.long())).to(self.device)\n",
    "        GGU_P1_k = torch.zeros(proxy_inds.shape[0]*num_global_neighbors*4, 1).to(self.device)\n",
    "#         GGU_P1_k = torch.ones(proxy_inds.shape[0]*num_global_neighbors*2, 1).to(self.device)\n",
    "        GGU_P1_k = nn.init.xavier_uniform_(GGU_P1_k)\n",
    "\n",
    "        return h, SGU_g, GGU_P1_g, GGU_P2_g, SGU_k, GGU_P1_k, GGU_P2_k\n",
    "    \n",
    "    def forward(self, graphs):\n",
    "        original_length = graphs.num_nodes()\n",
    "        indices = []    \n",
    "        edge_lengths = []\n",
    "        node_lengths = []\n",
    "        for graph in dgl.unbatch(graphs):\n",
    "            adj = graph.adjacency_matrix(transpose=False)._indices()\n",
    "            ind1 = adj[0]\n",
    "            ind2 = adj[1]\n",
    "            inds = [ind1, ind2]\n",
    "            edge_lengths.append(graph.num_edges())\n",
    "            node_lengths.append(graph.num_nodes())\n",
    "            indices.append(inds)\n",
    "        \n",
    "        h, SGU_g, GGU_P1_g, GGU_P2_g, SGU_k, GGU_P1_k, GGU_P2_k = self.GDG(graphs, indices, edge_lengths, node_lengths)\n",
    "\n",
    "        for ind, conv in enumerate(self.layers):\n",
    "            if ind < len(self.layers)-1:\n",
    "                SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k = conv(SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k, original_length, ind)\n",
    "            else:\n",
    "                best_scores = conv(SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k, original_length, ind, graphs)\n",
    "        return best_scores\n",
    "\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "        loss = criterion(pred, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "net_params = {}\n",
    "net_params['device'] = device\n",
    "net_params['in_dim'] = trainset[0][0].ndata['feat'][0].size(0)\n",
    "net_params['in_dim_edge'] = trainset[0][0].edata['feat'][0].size(0)\n",
    "net_params['residual'] = True\n",
    "net_params['hidden_dim'] = 100 \n",
    "net_params['out_dim'] = 100\n",
    "num_classes = len(np.unique(np.concatenate(trainset[:][1])))\n",
    "net_params['n_classes'] = num_classes\n",
    "net_params['L'] = 5  \n",
    "net_params['readout'] = \"mean\"\n",
    "net_params['layer_norm'] = True\n",
    "net_params['batch_norm'] = True\n",
    "net_params['in_feat_dropout'] = 0.0\n",
    "net_params['dropout'] = 0.01 \n",
    "net_params['edge_feat'] = True\n",
    "net_params['self_loop'] = False\n",
    "\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=27, shuffle=True, collate_fn=dataset.collate, drop_last=True)\n",
    "val_loader = DataLoader(valset, batch_size=27, shuffle=False, collate_fn=dataset.collate, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=27, shuffle=False, collate_fn=dataset.collate, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "torch.cuda.manual_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIGNet(net_params, num_classes, 3, device)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=0)                               \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_sparse(model, optimizer, device, data_loader, epoch):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_train_f1 = 0\n",
    "    nb_data = 0\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_graphs = batch_graphs.to(device)\n",
    "        batch_x = batch_graphs.ndata['feat'].to(device)  # num x feat\n",
    "        batch_e = batch_graphs.edata['feat'].to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_scores = model.forward(batch_graphs)\n",
    "        loss = model.loss(batch_scores, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_train_f1 += binary_f1_score(batch_scores, batch_labels)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_train_f1 /= (iter + 1)\n",
    "    \n",
    "    return epoch_loss, epoch_train_f1, optimizer\n",
    "\n",
    "\n",
    "def evaluate_network_sparse(model, device, data_loader, epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_f1 = 0\n",
    "    nb_data = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "            batch_graphs = batch_graphs.to(device)\n",
    "            batch_x = batch_graphs.ndata['feat'].to(device)\n",
    "            batch_e = batch_graphs.edata['feat'].to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            batch_scores = model.forward(batch_graphs)\n",
    "            loss = model.loss(batch_scores, batch_labels) \n",
    "            epoch_test_loss += loss.detach().item()\n",
    "            epoch_test_f1 += binary_f1_score(batch_scores, batch_labels)\n",
    "        epoch_test_loss /= (iter + 1)\n",
    "        epoch_test_f1 /= (iter + 1)\n",
    "        \n",
    "    return epoch_test_loss, epoch_test_f1\n",
    "\n",
    "def binary_f1_score(scores, targets):\n",
    "    \"\"\"Computes the F1 score using scikit-learn for binary class labels. \n",
    "    \n",
    "    Returns the F1 score for the positive class, i.e. labelled '1'.\n",
    "    \"\"\"\n",
    "    y_true = targets.cpu().numpy()\n",
    "    y_pred = scores.argmax(dim=1).cpu().numpy()\n",
    "    return f1_score(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = os.path.join('/root/tf-logs/', \"DATA_TSP\")    \n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "with tqdm(range(360)) as t:\n",
    "    for epoch in t:\n",
    "        t.set_description('Epoch %d' % epoch)\n",
    "        epoch_train_loss, epoch_train_f1, optimizer = train_epoch_sparse(model, optimizer, device, train_loader, epoch)\n",
    "        epoch_val_loss, epoch_val_f1 = evaluate_network_sparse(model, device, val_loader, epoch)\n",
    "        _, epoch_test_f1 = evaluate_network_sparse(model, device, test_loader, epoch)                \n",
    "        \n",
    "        start = time.time()\n",
    "        writer.add_scalar('train/_loss', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('val/_loss', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('train/_f1', epoch_train_f1, epoch)\n",
    "        writer.add_scalar('val/_f1', epoch_val_f1, epoch)\n",
    "        writer.add_scalar('test/_f1', epoch_test_f1, epoch)\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        \n",
    "        ckpt_dir = os.path.join(os.getcwd(), \"MODEL_TSP\")\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        torch.save(model.state_dict(), '{}.pkl'.format(ckpt_dir + \"/epoch_\" + str(epoch)))\n",
    "\n",
    "        t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0]['lr'],\n",
    "                      train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n",
    "                      train_acc=epoch_train_f1, val_acc=epoch_val_f1, \n",
    "                      test_acc=epoch_test_f1)    \n",
    "\n",
    "        scheduler.step(epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
