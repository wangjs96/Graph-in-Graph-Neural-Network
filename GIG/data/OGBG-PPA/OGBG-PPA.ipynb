{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import glob\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.functional import edge_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../') # go to root folder of the project\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import DatasetProcess\n",
    "from ogb.graphproppred import Evaluator\n",
    "\n",
    "dataset = DatasetProcess('ogbg-ppa')\n",
    "evaluator = Evaluator(name='ogbg-ppa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = dataset.get_idx_split() \n",
    "train_loader = DataLoader(dataset[split_idx['train']], batch_size=32, shuffle=True, collate_fn=dataset.collate_dgl)\n",
    "val_loader = DataLoader(dataset[split_idx['valid']], batch_size=32, shuffle=False, collate_fn=dataset.collate_dgl)\n",
    "test_loader = DataLoader(dataset[split_idx['test']], batch_size=32, shuffle=False, collate_fn=dataset.collate_dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "\n",
    "def act_layer(act_type, inplace=False, neg_slope=0.2, n_prelu=1):\n",
    "    act = act_type.lower()\n",
    "    \n",
    "    if act == 'relu':\n",
    "        layer = nn.ReLU(inplace)\n",
    "    elif act == 'leakyrelu':\n",
    "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
    "    elif act == 'prelu':\n",
    "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
    "    else:\n",
    "        raise NotImplementedError('activation layer [%s] is not found' % act)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "def norm_layer(norm_type, nc):\n",
    "    norm = norm_type.lower()\n",
    "\n",
    "    if norm == 'batch':\n",
    "        layer = nn.BatchNorm1d(nc, affine=True)\n",
    "    elif norm == 'layer':\n",
    "        layer = nn.LayerNorm(nc, elementwise_affine=True)\n",
    "    elif norm == 'instance':\n",
    "        layer = nn.InstanceNorm1d(nc, affine=False)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Normalization layer {norm} is not supported.')\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 act='relu',\n",
    "                 norm=None,\n",
    "                 dropout=0.,\n",
    "                 bias=True):\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(1, len(channels)):\n",
    "            layers.append(nn.Linear(channels[i - 1], channels[i], bias))\n",
    "            if i < len(channels) - 1:\n",
    "                if norm is not None and norm.lower() != 'none':\n",
    "                    layers.append(norm_layer(norm, channels[i]))\n",
    "                if act is not None and act.lower() != 'none':\n",
    "                    layers.append(act_layer(act))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        super(MLP, self).__init__(*layers)\n",
    "\n",
    "\n",
    "class MessageNorm(nn.Module):\n",
    "    def __init__(self, learn_scale=False):\n",
    "        super(MessageNorm, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([1.0]), requires_grad=learn_scale)\n",
    "\n",
    "    def forward(self, feats, msg, p=2):\n",
    "        msg = F.normalize(msg, p=2, dim=-1)\n",
    "        feats_norm = feats.norm(p=p, dim=-1, keepdim=True)\n",
    "        return msg * feats_norm * self.scale\n",
    "\n",
    "\n",
    "class GENConv(nn.Module):\n",
    "    r\"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: str\n",
    "        Name of ogb dataset.\n",
    "    in_dim: int\n",
    "        Size of input dimension.\n",
    "    out_dim: int\n",
    "        Size of output dimension.\n",
    "    aggregator: str\n",
    "        Type of aggregator scheme ('softmax', 'power'), default is 'softmax'.\n",
    "    beta: float\n",
    "        A continuous variable called an inverse temperature. Default is 1.0.\n",
    "    learn_beta: bool\n",
    "        Whether beta is a learnable variable or not. Default is False.\n",
    "    p: float\n",
    "        Initial power for power mean aggregation. Default is 1.0.\n",
    "    learn_p: bool\n",
    "        Whether p is a learnable variable or not. Default is False.\n",
    "    msg_norm: bool\n",
    "        Whether message normalization is used. Default is False.\n",
    "    learn_msg_scale: bool\n",
    "        Whether s is a learnable scaling factor or not in message normalization. Default is False.\n",
    "    norm: str\n",
    "        Type of ('batch', 'layer', 'instance') norm layer in MLP layers. Default is 'batch'.\n",
    "    mlp_layers: int\n",
    "        The number of MLP layers. Default is 1.\n",
    "    eps: float\n",
    "        A small positive constant in message construction function. Default is 1e-7.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 aggregator='softmax',\n",
    "                 beta=1.0,\n",
    "                 learn_beta=False,\n",
    "                 p=1.0,\n",
    "                 learn_p=False,\n",
    "                 msg_norm=False,\n",
    "                 learn_msg_scale=False,\n",
    "                 norm='batch',\n",
    "                 mlp_layers=1,\n",
    "                 eps=1e-7):\n",
    "        super(GENConv, self).__init__()\n",
    "        \n",
    "        self.aggr = aggregator\n",
    "        self.eps = eps\n",
    "\n",
    "        channels = [in_dim]\n",
    "        for i in range(mlp_layers - 1):\n",
    "            channels.append(in_dim * 2)\n",
    "        channels.append(out_dim)\n",
    "\n",
    "        self.mlp = MLP(channels, norm=norm)\n",
    "        self.msg_norm = MessageNorm(learn_msg_scale) if msg_norm else None\n",
    "\n",
    "        self.beta = nn.Parameter(torch.Tensor([beta]), requires_grad=True) if learn_beta and self.aggr == 'softmax' else beta\n",
    "        self.p = nn.Parameter(torch.Tensor([p]), requires_grad=True) if learn_p else p\n",
    "\n",
    "        if dataset == 'ogbg-molhiv':\n",
    "            self.edge_encoder = BondEncoder(in_dim)\n",
    "        elif dataset == 'ogbg-ppa':\n",
    "            self.edge_encoder = nn.Linear(in_dim, in_dim)\n",
    "        else:\n",
    "            raise ValueError(f'Dataset {dataset} is not supported.')\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats):\n",
    "        with g.local_scope():\n",
    "            # Node and edge feature dimension need to match.\n",
    "            g.ndata['h'] = node_feats\n",
    "            g.edata['h'] = self.edge_encoder(edge_feats)\n",
    "            g.apply_edges(fn.u_add_e('h', 'h', 'm'))\n",
    "\n",
    "            if self.aggr == 'softmax':\n",
    "                g.edata['m'] = F.relu(g.edata['m']) + self.eps\n",
    "                g.edata['a'] = edge_softmax(g, g.edata['m'] * self.beta)\n",
    "                g.update_all(lambda edge: {'x': edge.data['m'] * edge.data['a']},\n",
    "                             fn.sum('x', 'm'))\n",
    "            \n",
    "            elif self.aggr == 'power':\n",
    "                minv, maxv = 1e-7, 1e1\n",
    "                torch.clamp_(g.edata['m'], minv, maxv)\n",
    "                g.update_all(lambda edge: {'x': torch.pow(edge.data['m'], self.p)},\n",
    "                             fn.mean('x', 'm'))\n",
    "                torch.clamp_(g.ndata['m'], minv, maxv)\n",
    "                g.ndata['m'] = torch.pow(g.ndata['m'], self.p)\n",
    "            \n",
    "            else:\n",
    "                raise NotImplementedError(f'Aggregator {self.aggr} is not supported.')\n",
    "            \n",
    "            if self.msg_norm is not None:\n",
    "                g.ndata['m'] = self.msg_norm(node_feats, g.ndata['m'])\n",
    "            \n",
    "            feats = node_feats + g.ndata['m']\n",
    "            \n",
    "            return self.mlp(feats)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class DeeperGCN(nn.Module):\n",
    "    r\"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: str\n",
    "        Name of ogb dataset.\n",
    "    node_feat_dim: int\n",
    "        Size of node feature dimension.\n",
    "    edge_feat_dim: int\n",
    "        Size of edge feature dimension.\n",
    "    hid_dim: int\n",
    "        Size of hidden dimension.\n",
    "    out_dim: int\n",
    "        Size of output dimension.\n",
    "    num_layers: int\n",
    "        Number of graph convolutional layers.\n",
    "    dropout: float\n",
    "        Dropout rate. Default is 0.\n",
    "    norm: str\n",
    "        Type of ('batch', 'layer', 'instance') norm layer in MLP layers. Default is 'batch'.\n",
    "    pooling: str\n",
    "        Type of ('sum', 'mean', 'max') pooling layer. Default is 'mean'.\n",
    "    beta: float\n",
    "        A continuous variable called an inverse temperature. Default is 1.0.\n",
    "    lean_beta: bool\n",
    "        Whether beta is a learnable weight. Default is False.\n",
    "    aggr: str\n",
    "        Type of aggregator scheme ('softmax', 'power'). Default is 'softmax'.\n",
    "    mlp_layers: int\n",
    "        Number of MLP layers in message normalization. Default is 1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 node_feat_dim,\n",
    "                 edge_feat_dim,\n",
    "                 hid_dim,\n",
    "                 out_dim,\n",
    "                 num_layers,\n",
    "                 update_type=1,\n",
    "                 dropout=0.,\n",
    "                 norm='batch',\n",
    "                 pooling='mean',\n",
    "                 beta=1.0,\n",
    "                 learn_beta=False,\n",
    "                 aggr='softmax',\n",
    "                 mlp_layers=1):\n",
    "        super(DeeperGCN, self).__init__()\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.gcns = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.update_type = update_type\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            conv = GENConv(dataset=dataset,\n",
    "                           in_dim=hid_dim,\n",
    "                           out_dim=hid_dim,\n",
    "                           aggregator=aggr,\n",
    "                           beta=beta,\n",
    "                           learn_beta=learn_beta,\n",
    "                           mlp_layers=mlp_layers,\n",
    "                           norm=norm)\n",
    "            \n",
    "            self.gcns.append(conv)\n",
    "            self.norms.append(norm_layer(norm, hid_dim))\n",
    "\n",
    "        self.node_encoder = nn.Linear(node_feat_dim, hid_dim)\n",
    "        self.edge_encoder = nn.Linear(edge_feat_dim, hid_dim)\n",
    "\n",
    "\n",
    "#         if pooling == 'sum':\n",
    "#             self.pooling = SumPooling()\n",
    "#         elif pooling == 'mean':\n",
    "#             self.pooling = AvgPooling()\n",
    "#         elif pooling == 'max':\n",
    "#             self.pooling = MaxPooling()\n",
    "#         else:\n",
    "#             raise NotImplementedError(f'{pooling} is not supported.')\n",
    "        \n",
    "        self.output = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, g, hv, he):\n",
    "        hv = self.node_encoder(hv)\n",
    "        he = self.node_encoder(he)\n",
    "        \n",
    "        with g.local_scope():\n",
    "            for layer in range(self.num_layers):\n",
    "                hv1 = self.norms[layer](hv)\n",
    "                hv1 = F.relu(hv1)\n",
    "                hv1 = F.dropout(hv1, p=self.dropout, training=self.training)\n",
    "                hv = self.gcns[layer](g, hv1, he) + hv\n",
    "\n",
    "#             h_g = self.pooling(g, hv)\n",
    "\n",
    "            return hv, he  \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGLayer(nn.Module):\n",
    "    def __init__(self, net_params, device, is_last_layer):\n",
    "        super(GIGLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.is_last_layer = is_last_layer\n",
    "        SGU_params = net_params.copy()\n",
    "        GGU_P1_params = net_params.copy()\n",
    "        GGU_P2_params = net_params.copy()\n",
    "        SGU_params['L'] = 2\n",
    "        GGU_P1_params['L'] = 1\n",
    "        GGU_P2_params['L'] = 1\n",
    "        GGU_P1_params['in_dim'] = net_params['hidden_dim']\n",
    "        GGU_P2_params['in_dim'] = net_params['hidden_dim']\n",
    "        \n",
    "        self.SGU = DeeperGCN(dataset='ogbg-ppa',\n",
    "                      node_feat_dim=7,\n",
    "                      edge_feat_dim=7,\n",
    "                      hid_dim=100,\n",
    "                      out_dim=net_params['n_classes'],\n",
    "                      num_layers=net_params['L']).to(device)    \n",
    "        self.GGU_Part1 = DeeperGCN(dataset='ogbg-ppa',\n",
    "                      node_feat_dim=7,\n",
    "                      edge_feat_dim=7,\n",
    "                      hid_dim=100,\n",
    "                      out_dim=net_params['n_classes'],\n",
    "                      num_layers=GGU_P1_params['L']).to(device) \n",
    "        self.GGU_Part2 = DeeperGCN(dataset='ogbg-ppa',\n",
    "                      node_feat_dim=7,\n",
    "                      edge_feat_dim=7,\n",
    "                      hid_dim=100,\n",
    "                      out_dim=net_params['n_classes'],\n",
    "                      num_layers=GGU_P2_params['L']).to(device) \n",
    "        \n",
    "    \n",
    "    def forward(self, SGU_graph, GGU_P1_graph, GGU_P2_graph, h, SGU_e, GGU_P1_e, GGU_P2_e, stage):\n",
    "        h1, e1 = self.SGU(SGU_graph, h, SGU_e)\n",
    "        h2, e2 = self.GGU_Part1(GGU_P1_graph, h1, GGU_P1_e)\n",
    "        h3, e3 = self.GGU_Part2(GGU_P2_graph, h2, GGU_P2_e)\n",
    "\n",
    "        if not self.is_last_layer:\n",
    "            return SGU_graph, GGU_P1_graph, GGU_P2_graph, h3, e1, e2, e3\n",
    "        else:\n",
    "            SGU_graph.ndata['h'] = h3\n",
    "            hg = dgl.mean_nodes(SGU_graph, 'h')\n",
    "            return hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGNet(nn.Module):\n",
    "    def __init__(self, net_params, n_layers, num_neighbors, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.layers = nn.ModuleList([GIGLayer(net_params, device, False) for _ in range(n_layers-1)])\n",
    "        self.layers.append(GIGLayer(net_params, device, True))\n",
    "        self.edge_encoder = nn.Linear(net_params['in_dim'], net_params['hidden_dim'])\n",
    "        self.node_encoder = nn.Linear(net_params['in_dim'], net_params['hidden_dim'])\n",
    "#         self.MLP_layer = MLPReadout(net_params['out_dim'], net_params['n_classes'])\n",
    "        self.output = nn.Linear(net_params['hidden_dim'], net_params['n_classes'])\n",
    "\n",
    "    #Obtain global neighbors\n",
    "    def global_neighbors(self, h, num_global_neighbors):\n",
    "        h = h.to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', h, h.transpose(0, 1)).to(self.device)\n",
    "        diag = torch.diag(si).to(self.device)\n",
    "        a_diag = torch.diag_embed(diag).to(self.device)\n",
    "        si = (si - a_diag).to(self.device)\n",
    "        _, target_inds = si.topk(k=num_global_neighbors, dim=1, largest=True)\n",
    "        return target_inds.to(self.device)\n",
    "    \n",
    "    #GIG data generation module\n",
    "    def GDG(self, graphs, indices, edge_lengths, node_lengths):\n",
    "        global_src = torch.zeros((0)).to(self.device)\n",
    "        global_tgt = torch.zeros((0)).to(self.device)\n",
    "        node_index = 0\n",
    "        edge_index = 0\n",
    "        SGU_graphs = []\n",
    "        GGU_P2_graphs = []\n",
    "        proxy_inds = []\n",
    "        proxy_vertices = torch.zeros((0)).to(self.device)\n",
    "        #graph construction in DGL for SGU and GGU part 2\n",
    "        for ind, indice in enumerate(indices):\n",
    "            node_length = node_lengths[ind]\n",
    "            edge_length = edge_lengths[ind]\n",
    "            \n",
    "            #Vertex and edge features from current sub-graph\n",
    "            h = graphs.ndata['feat'][node_index:node_index+node_length, :].to(self.device)\n",
    "            e = graphs.edata['feat'][edge_index:edge_index+edge_length, :].to(self.device)\n",
    "            \n",
    "            #Source and target for SGU module\n",
    "            src1 = torch.arange(node_length).to(self.device)\n",
    "            tgt1 = torch.full([node_length,], node_length).to(self.device)\n",
    "            src1 = torch.cat((indice[0].to(self.device), src1))\n",
    "            tgt1 = torch.cat((indice[1].to(self.device), tgt1))\n",
    "            \n",
    "            #Source and target for GGU part 2\n",
    "            src3 = torch.full([node_length,], node_length).to(self.device)\n",
    "            tgt3 = torch.arange(node_length).to(self.device)\n",
    "            \n",
    "            #SGU graph\n",
    "            SGU_g = dgl.graph((src1, tgt1)).to(self.device)\n",
    "            \n",
    "            #GGU part 2 graph\n",
    "            GGU_P2_g = dgl.graph((src3, tgt3)).to(self.device)\n",
    "            \n",
    "            #Proxy vertex feature initilization \n",
    "            proxy_vertex = h.mean(dim=0).reshape(1,100).to(self.device)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex), dim=0)\n",
    "            \n",
    "            #Vertex and edge features construction\n",
    "            SGU_node_features = torch.cat((h, proxy_vertex), dim=0)\n",
    "            GGU_P1_node_features = torch.cat((h, proxy_vertex), dim=0)\n",
    "            SGU_edge_features = torch.cat((e, torch.randn(h.shape[0],100).to(self.device)))\n",
    "            GGU_P2_edge_features = torch.randn(h.shape[0],100).to(self.device)\n",
    "            SGU_g.ndata['feat'] = SGU_node_features\n",
    "            GGU_P2_g.ndata['feat'] = GGU_P1_node_features\n",
    "            SGU_g.edata['feat'] = SGU_edge_features\n",
    "            GGU_P2_g.edata['feat'] = GGU_P2_edge_features\n",
    "            \n",
    "            #Update indexes\n",
    "            node_index += node_length\n",
    "            edge_index += edge_length\n",
    "            \n",
    "            #Append graphs into corresponding sets\n",
    "            SGU_graphs.append(SGU_g)\n",
    "            GGU_P2_graphs.append(GGU_P2_g)\n",
    "            \n",
    "            if ind == 0:\n",
    "                current_ind = node_length\n",
    "            else:\n",
    "                current_ind = current_ind + node_length + 1\n",
    "            proxy_inds.append(current_ind)\n",
    "            \n",
    "        #Combine single graphs to form a new DGL graph\n",
    "        SGU_graphs = dgl.batch(SGU_graphs).to(self.device)\n",
    "        GGU_P2_graphs = dgl.batch(GGU_P2_graphs).to(self.device)\n",
    "        \n",
    "        #Obtain edge features of SGU and GGU part 2\n",
    "        SGU_k = SGU_graphs.edata['feat']\n",
    "        GGU_P2_k = GGU_P2_graphs.edata['feat']\n",
    "        \n",
    "        #graph construction in DGL for GGU part 1\n",
    "        num_global_neighbors = self.num_neighbors\n",
    "        proxy_inds = torch.tensor(proxy_inds)\n",
    "        target_inds = self.global_neighbors(proxy_vertices, num_global_neighbors)\n",
    "        for ind, i in enumerate(proxy_inds):\n",
    "            src1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt1 = proxy_inds[target_inds[ind,:]].to(self.device)\n",
    "            src2 = proxy_inds[target_inds[ind,:]].to(self.device)\n",
    "            tgt2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            \n",
    "            src = torch.cat((src1, src2))\n",
    "            tgt = torch.cat((tgt1, tgt2))\n",
    "            \n",
    "            global_src = torch.cat((global_src, src))\n",
    "            global_tgt = torch.cat((global_tgt, tgt)) \n",
    "        \n",
    "        #Ensure the number of vertex matches with original graph\n",
    "        last_src = torch.tensor([SGU_graphs.num_nodes()-1]).to(self.device)\n",
    "        last_tgt = torch.tensor([SGU_graphs.num_nodes()-1]).to(self.device)\n",
    "        global_src = torch.cat((global_src, last_src))\n",
    "        global_tgt = torch.cat((global_tgt, last_tgt))\n",
    "          \n",
    "        #Form new DGL graph for GGU part 1\n",
    "        GGU_P1_graphs = dgl.graph((global_src.long(), global_tgt.long())).to(self.device)\n",
    "        GGU_P1_k = torch.randn(proxy_inds.shape[0]*num_global_neighbors*2+1, 100).to(self.device)\n",
    "\n",
    "        return SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, SGU_k, GGU_P1_k, GGU_P2_k\n",
    "    \n",
    "    def forward(self, graphs):\n",
    "        indices = []    \n",
    "        edge_lengths = []\n",
    "        node_lengths = []\n",
    "        for graph in dgl.unbatch(graphs):\n",
    "            adj = graph.adjacency_matrix(transpose=False)._indices()\n",
    "            ind1 = adj[0]\n",
    "            ind2 = adj[1]\n",
    "            inds = [ind1, ind2]\n",
    "            edge_lengths.append(graph.num_edges())\n",
    "            node_lengths.append(graph.num_nodes())\n",
    "            indices.append(inds)\n",
    "            \n",
    "            \n",
    "        graphs.edata['h'] = graphs.edata['feat']\n",
    "        graphs.update_all(fn.copy_e('h', 'm'), fn.sum('m', 'h1'))\n",
    "        graphs.ndata['feat'] = self.node_encoder(graphs.ndata['h1'])\n",
    "        graphs.edata['feat'] = self.edge_encoder(graphs.edata['feat'])\n",
    "        \n",
    "        \n",
    "        #GIG data\n",
    "        SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, SGU_k, GGU_P1_k, GGU_P2_k = \\\n",
    "        self.GDG(graphs, indices, edge_lengths, node_lengths)\n",
    "        h = SGU_graphs.ndata['feat']\n",
    "        \n",
    "        #Forward propagation\n",
    "        for ind, conv in enumerate(self.layers):\n",
    "            if ind < len(self.layers)-1:\n",
    "                SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, h, SGU_k, GGU_P1_k, GGU_P2_k = \\\n",
    "                conv(SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, h, SGU_k, GGU_P1_k, GGU_P2_k, ind)\n",
    "            else:\n",
    "                best_scores = \\\n",
    "                conv(SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, h, SGU_k, GGU_P1_k, GGU_P2_k, ind)\n",
    "        return self.output(best_scores)\n",
    "\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "        loss = criterion(pred, label.long())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "net_params = {}\n",
    "net_params['device'] = device\n",
    "net_params['gated'] = True \n",
    "net_params['in_dim'] = dataset[split_idx['train']][0][0].edata['feat'].size()[-1]\n",
    "net_params['in_dim_edge'] = dataset[split_idx['train']][0][0].edata['feat'].size()[-1]\n",
    "net_params['residual'] = True\n",
    "net_params['hidden_dim'] = 105 \n",
    "net_params['out_dim'] = 105\n",
    "num_classes = int(dataset.num_classes)\n",
    "net_params['n_classes'] = num_classes\n",
    "net_params['n_heads'] = -1\n",
    "net_params['L'] = 4  \n",
    "net_params['readout'] = 'mean'\n",
    "net_params['layer_norm'] = True\n",
    "net_params['batch_norm'] = True\n",
    "net_params['in_feat_dropout'] = 0.1\n",
    "net_params['dropout'] = 0.05 \n",
    "net_params['edge_feat'] = True\n",
    "net_params['self_loop'] = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "torch.cuda.manual_seed(41)\n",
    "    \n",
    "model = GIGNet(net_params, 2, 6, device)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_sparse(model, optimizer, device, data_loader):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    for g, labels in data_loader:\n",
    "        g = g.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model.forward(g)\n",
    "        loss = F.nll_loss(logits, labels.squeeze(1).long())\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum(train_loss) / len(train_loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_network_sparse(model, device, data_loader, evaluator):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for g, labels in data_loader:\n",
    "        g = g.to(device)\n",
    "        logits = model.forward(g)\n",
    "        y_true.append(labels.detach().cpu())\n",
    "        y_pred.append(logits.argmax(dim=-1, keepdim=True).detach().cpu())\n",
    "    \n",
    "    y_true = torch.cat(y_true, dim=0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "\n",
    "    return evaluator.eval({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred\n",
    "    })['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = os.path.join('/root/tf-logs/', \"DATA_OGBG_PPA\")    \n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "with tqdm(range(360)) as t:\n",
    "    for epoch in t:\n",
    "        t.set_description('Epoch %d' % epoch)\n",
    "        epoch_train_loss = train_epoch_sparse(model, optimizer, device, train_loader)\n",
    "        epoch_val_acc = evaluate_network_sparse(model, device, val_loader, evaluator)\n",
    "        epoch_test_acc = evaluate_network_sparse(model, device, test_loader, evaluator)                \n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        writer.add_scalar('train/_loss', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('val/_acc', epoch_val_acc, epoch)\n",
    "        writer.add_scalar('test/_acc', epoch_test_acc, epoch)\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        \n",
    "        ckpt_dir = os.path.join(os.getcwd(), \"MODEL_OGBG_PPA\")\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        torch.save(model.state_dict(), '{}.pkl'.format(ckpt_dir + \"/epoch_\" + str(epoch)))\n",
    "   \n",
    "        t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0]['lr'],\n",
    "                      train_loss=epoch_train_loss, \n",
    "                      val_acc=epoch_val_acc, \n",
    "                      test_acc=epoch_test_acc)    \n",
    "\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
