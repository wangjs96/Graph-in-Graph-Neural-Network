{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "from scipy import sparse as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../') \n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.superpixels import SuperPixDatasetDGL \n",
    "\n",
    "from data.data import LoadData\n",
    "from torch.utils.data import DataLoader\n",
    "from data.superpixels import SuperPixDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'SBM_PATTERN'\n",
    "dataset = LoadData(DATASET_NAME) \n",
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "from bisect import bisect_right\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "\n",
    "class MLPReadout(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, L=3): \n",
    "        super().__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim//2**l , input_dim//2**(l+1) , bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim//2**L , output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "\n",
    "    def act(self, x):\n",
    "        return x*(torch.tanh(F.softplus(x)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = F.leaky_relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedGCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, batch_norm, residual=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_dim\n",
    "        self.out_channels = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        if input_dim != output_dim:\n",
    "            self.residual = False\n",
    "        \n",
    "        self.A = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.B = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.C = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.D = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.E = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "    \n",
    "    def act(self, x):\n",
    "        return x*(torch.tanh(F.softplus(x)))\n",
    "    \n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        Bh_j = edges.src['Bh']    \n",
    "        e_ij = edges.data['Ce'] +  edges.src['Dh'] + edges.dst['Eh'] \n",
    "        edges.data['e'] = e_ij\n",
    "        return {'Bh_j' : Bh_j, 'e_ij' : e_ij}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        Ah_i = nodes.data['Ah']\n",
    "        Bh_j = nodes.mailbox['Bh_j']\n",
    "        e = nodes.mailbox['e_ij'] \n",
    "        sigma_ij = torch.sigmoid(e) \n",
    "        h = Ah_i + torch.sum( sigma_ij * Bh_j, dim=1 ) / ( torch.sum( sigma_ij, dim=1 ) + 1e-6 )      \n",
    "        return {'h' : h}\n",
    "    \n",
    "    def forward(self, g, h, e):\n",
    "        h_in = h \n",
    "        e_in = e \n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) \n",
    "        g.ndata['Bh'] = self.B(h) \n",
    "        g.ndata['Dh'] = self.D(h)\n",
    "        g.ndata['Eh'] = self.E(h) \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e) \n",
    "        \n",
    "        g.apply_edges(fn.u_add_v('Dh', 'Eh', 'DEh'))\n",
    "        g.edata['e'] = g.edata['DEh'] + g.edata['Ce']\n",
    "        g.edata['sigma'] = torch.sigmoid(g.edata['e'])\n",
    "        g.update_all(fn.u_mul_e('Bh', 'sigma', 'm'), fn.sum('m', 'sum_sigma_h'))\n",
    "        g.update_all(fn.copy_e('sigma', 'm'), fn.sum('m', 'sum_sigma'))\n",
    "        g.ndata['h'] = g.ndata['Ah'] + g.ndata['sum_sigma_h'] / (g.ndata['sum_sigma'] + 1e-6)\n",
    "\n",
    "        h = g.ndata['h'] # result of graph convolution\n",
    "        e = g.edata['e'] # result of graph convolution\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            h = self.bn_node_h(h) # batch normalization  \n",
    "            e = self.bn_node_e(e) # batch normalization  \n",
    "        \n",
    "        # h = F.leaky_relu(h) # non-linear activation\n",
    "        # e = F.leaky_relu(e) # non-linear activation\n",
    "        h = self.act(h) # non-linear activation\n",
    "        e = self.act(e) # non-linear activation\n",
    "        \n",
    "        if self.residual:\n",
    "            h = h_in + h # residual connection\n",
    "            e = e_in + e # residual connection\n",
    "        \n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        e = F.dropout(e, self.dropout, training=self.training)\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}(in_channels={}, out_channels={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels)\n",
    "\n",
    "    \n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, net_params, update_type):\n",
    "        super().__init__()\n",
    "        in_dim = net_params['in_dim']\n",
    "        in_dim_edge = 1\n",
    "        hidden_dim = net_params['hidden_dim']\n",
    "        out_dim = net_params['out_dim']\n",
    "        n_classes = net_params['n_classes']\n",
    "        dropout = net_params['dropout']\n",
    "        n_layers = net_params['L']\n",
    "        self.readout = net_params['readout']\n",
    "        self.batch_norm = net_params['batch_norm']\n",
    "        self.residual = net_params['residual']\n",
    "        self.edge_feat = net_params['edge_feat']\n",
    "        self.device = net_params['device']\n",
    "        self.update_type = update_type\n",
    "        self.pos_enc = net_params['pos_enc']\n",
    "        if self.pos_enc:\n",
    "            pos_enc_dim = net_params['pos_enc_dim']\n",
    "            self.embedding_pos_enc = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n",
    "                                                    self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n",
    "        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.batch_norm, self.residual))\n",
    "\n",
    "        \n",
    "    def forward(self, g, h, e, stage, h_pos_enc=None):\n",
    "\n",
    "        if self.pos_enc:\n",
    "            h_pos_enc = self.embedding_pos_enc(h_pos_enc.float()) \n",
    "            h = h + h_pos_enc \n",
    "        \n",
    "        for conv in self.layers:\n",
    "            h, e = conv(g, h, e)\n",
    "        return h, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGLayer(nn.Module):\n",
    "    def __init__(self, net_params, device, is_last_layer, embedding_h1, embedding_h2, embedding_e1, embedding_e2):\n",
    "        super(GIGLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.is_last_layer = is_last_layer\n",
    "        SGU_params = net_params.copy()\n",
    "        GGU_P1_params = net_params.copy()\n",
    "        GGU_P2_params = net_params.copy()\n",
    "        SGU_params['L'] = 10\n",
    "        GGU_P1_params['L'] = 3\n",
    "        GGU_P2_params['L'] = 3\n",
    "        GGU_P1_params['in_dim'] = net_params['hidden_dim']\n",
    "        GGU_P2_params['in_dim'] = net_params['hidden_dim']\n",
    "        self.SGU = GNN(SGU_params, 1)\n",
    "        self.GGU_Part1 = GNN(GGU_P1_params, 2)\n",
    "        self.GGU_Part2 = GNN(GGU_P2_params, 2)\n",
    "        self.MLP_layer = MLPReadout(net_params['out_dim'], net_params['n_classes'])\n",
    "        self.embedding_h1 = embedding_h1\n",
    "        self.embedding_h2 = embedding_h2\n",
    "        self.embedding_e1 = embedding_e1\n",
    "        self.embedding_e2 = embedding_e2\n",
    "    \n",
    "    def forward(self, SGU_graph, GGU_P1_graph, GGU_P2_graph, h, SGU_e, GGU_P1_e, GGU_P2_e, original_length, stage, batch_pos_enc):\n",
    "        if stage == 0:\n",
    "            h = self.embedding_h1(h)\n",
    "            SGU_e = self.embedding_e1(SGU_e)\n",
    "        h1, e1 = self.SGU(SGU_graph, h, SGU_e, stage, batch_pos_enc)\n",
    "\n",
    "        if stage == 0:\n",
    "            h1 = self.embedding_h2(h1)\n",
    "            GGU_P1_e = self.embedding_e2(GGU_P1_e)\n",
    "        h2, e2 = self.GGU_Part1(GGU_P1_graph, h1, GGU_P1_e, stage, batch_pos_enc)\n",
    "\n",
    "        if stage == 0:\n",
    "            h2 = self.embedding_h2(h2)\n",
    "            GGU_P2_e = self.embedding_e2(GGU_P2_e)\n",
    "        h3, e3 = self.GGU_Part2(GGU_P2_graph, h2, GGU_P2_e, stage, batch_pos_enc)\n",
    "\n",
    "        if not self.is_last_layer:\n",
    "            return SGU_graph, GGU_P1_graph, GGU_P2_graph, h3, e1, e2, e3\n",
    "        else:\n",
    "            h_r = h3[:original_length, :]\n",
    "            return self.MLP_layer(h_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGNet(nn.Module):\n",
    "    def __init__(self, net_params, n_classes, n_layers, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_classes = n_classes\n",
    "        self.embedding_h1 = nn.Linear(net_params['in_dim'], net_params['hidden_dim'])\n",
    "        self.embedding_h2 = nn.Linear(net_params['hidden_dim'], net_params['hidden_dim'])\n",
    "        self.embedding_e1 = nn.Linear(1, net_params['hidden_dim'])\n",
    "        self.embedding_e2 = nn.Linear(1, net_params['hidden_dim'])\n",
    "        self.layers = nn.ModuleList([GIGLayer(net_params, device, False, self.embedding_h1, self.embedding_h2, self.embedding_e1, self.embedding_e2) for _ in range(n_layers-1)])\n",
    "        self.layers.append(GIGLayer(net_params, device, True, self.embedding_h1, self.embedding_h2, self.embedding_e1, self.embedding_e2))\n",
    "        self.embedding_h = nn.Embedding(net_params['in_dim_node'], net_params['in_dim'])\n",
    "        self.in_dim = net_params['in_dim']\n",
    "        self.pos_enc_dim = net_params['pos_enc_dim']\n",
    "\n",
    "    def positional_encoding(self, g, pos_enc_dim):\n",
    "        # A = g.adjacency_matrix_scipy(return_edge_ids=False).astype(float)\n",
    "        A = g.adj_external().to_dense()\n",
    "        N = sp.diags(dgl.backend.asnumpy(g.in_degrees()).clip(1) ** -0.5, dtype=float)\n",
    "        L = sp.eye(g.number_of_nodes()) - N * A * N\n",
    "        \n",
    "        EigVal, EigVec = sp.linalg.eigs(L, k=pos_enc_dim+1, which='SR', tol=1e-2) # for 40 PEs\n",
    "        EigVec = EigVec[:, EigVal.argsort()] # increasing order\n",
    "        return torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).to(self.device).float() \n",
    "    \n",
    "    \n",
    "    \n",
    "    def proxy_edge_construction(self, proxy_vertex, sub_vertex, num_proxy_edges):\n",
    "        sub_vertex = sub_vertex.permute(1,0).to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', proxy_vertex, sub_vertex).to(self.device)\n",
    "        _, target_inds_sim = si.topk(k=num_proxy_edges, dim=1, largest=True)\n",
    "        _, target_inds_nonsim = si.topk(k=num_proxy_edges, dim=1, largest=False)\n",
    "        return target_inds_nonsim.squeeze(0).to(self.device), target_inds_sim.squeeze(0).to(self.device)\n",
    "\n",
    "    def global_neighbors(self, h, num_global_neighbors):\n",
    "        h = h.to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', h, h.transpose(0, 1)).to(self.device)\n",
    "        diag = torch.diag(si).to(self.device)\n",
    "        a_diag = torch.diag_embed(diag).to(self.device)\n",
    "        si = (si - a_diag).to(self.device)\n",
    "        _, target_inds_sim = si.topk(k=num_global_neighbors, dim=1, largest=True)\n",
    "        _, target_inds_nonsim = si.topk(k=num_global_neighbors, dim=1, largest=False)\n",
    "        return target_inds_sim.to(self.device), target_inds_nonsim.to(self.device)\n",
    "    \n",
    "    \n",
    "    def GDG(self, graphs, indices, edge_lengths, node_lengths, batch_pos_enc):\n",
    "        global_src = torch.zeros((0)).to(self.device)\n",
    "        global_tgt = torch.zeros((0)).to(self.device)\n",
    "        global_src_pos = torch.zeros((0)).to(self.device)\n",
    "        global_tgt_pos = torch.zeros((0)).to(self.device)\n",
    "        src1 = torch.zeros((0)).to(self.device)\n",
    "        tgt1 = torch.zeros((0)).to(self.device)\n",
    "        src3 = torch.zeros((0)).to(self.device)\n",
    "        tgt3 = torch.zeros((0)).to(self.device)\n",
    "        node_index = 0\n",
    "        current_ind = graphs.ndata['feat'].shape[0]\n",
    "        proxy_inds = []\n",
    "        proxy_vertices = torch.zeros((0)).to(self.device)\n",
    "        proxy_vertices_gi_lo = torch.zeros((0)).to(self.device)\n",
    "        proxy_edge_counter = 0\n",
    "        for ind, indice in enumerate(indices):\n",
    "            node_length = node_lengths[ind]\n",
    "            edge_length = edge_lengths[ind]\n",
    "            \n",
    "            h = graphs.ndata['feat'][node_index:node_index+node_length, :].to(self.device)\n",
    "            \n",
    "            proxy_vertex = h.mean(dim=0).reshape(1, self.in_dim).to(self.device)\n",
    "            proxy_vertex_li_go = nn.init.xavier_uniform_(proxy_vertex)\n",
    "            proxy_vertex_gi_lo = nn.init.xavier_uniform_(proxy_vertex)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex_li_go), dim=0)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex_gi_lo), dim=0)\n",
    "            proxy_vertices_gi_lo = torch.cat((proxy_vertices_gi_lo, proxy_vertex_gi_lo), dim=0)\n",
    "            \n",
    "            proxy_node_length = int(node_length * 0.1)\n",
    "            proxy_edge_counter += proxy_node_length\n",
    "            \n",
    "            \n",
    "            src1_sub, tgt3_sub = self.proxy_edge_construction(proxy_vertex, h, proxy_node_length)\n",
    "            src1_sub = src1_sub + node_index\n",
    "            tgt3_sub = tgt3_sub + node_index\n",
    "            tgt1_sub = torch.full([proxy_node_length,], current_ind).to(self.device)   \n",
    "            \n",
    "            src3_sub = torch.cat((torch.full([proxy_node_length,], current_ind+1).to(self.device), torch.tensor([current_ind]).to(self.device)))\n",
    "            tgt3_sub = torch.cat((tgt3_sub, torch.tensor([current_ind+1]).to(self.device)))\n",
    "            \n",
    "            \n",
    "            src1 = torch.cat((src1, src1_sub))\n",
    "            tgt1 = torch.cat((tgt1, tgt1_sub))\n",
    "            src3 = torch.cat((src3, src3_sub))\n",
    "            tgt3 = torch.cat((tgt3, tgt3_sub))\n",
    "            \n",
    "            \n",
    "            node_index += node_length\n",
    "            proxy_inds.append(current_ind+1)\n",
    "            current_ind += 2\n",
    "        \n",
    "        src1 = torch.cat((graphs.adj().indices()[0].to(self.device), src1))\n",
    "        tgt1 = torch.cat((graphs.adj().indices()[1].to(self.device), tgt1))\n",
    "        SGU_g = dgl.graph((src1.long(), tgt1.long())).to(self.device)\n",
    "        SGU_g.add_nodes(1)\n",
    "        GGU_P2_g = dgl.graph((src3.long(), tgt3.long())).to(self.device)\n",
    "        \n",
    "        h = torch.cat((graphs.ndata['feat'], proxy_vertices))\n",
    "        SGU_k = torch.cat((graphs.edata['feat'], torch.ones(proxy_edge_counter, 1).to(self.device)))\n",
    "        SGU_k = nn.init.xavier_uniform_(SGU_k)\n",
    "        GGU_P2_k = torch.randn(proxy_edge_counter+len(indices), 1).to(self.device)\n",
    "#         GGU_P2_k = torch.ones(graphs.num_nodes(), 1).to(self.device)\n",
    "        GGU_P2_k = nn.init.xavier_uniform_(GGU_P2_k)\n",
    "        \n",
    "        num_global_neighbors = 9\n",
    "        proxy_inds = torch.tensor(proxy_inds).to(self.device)     \n",
    "        proxy_inds_new = torch.arange(len(node_lengths)*2).to(self.device) \n",
    "        num_global_neighbors = int(num_global_neighbors/2)\n",
    "        target_inds_similar, target_inds_nonsimilar = self.global_neighbors(proxy_vertices_gi_lo, num_global_neighbors)\n",
    " \n",
    "\n",
    "        for ind, i in enumerate(proxy_inds):\n",
    "            src1_1 = proxy_inds[target_inds_similar[ind,:]].to(self.device)\n",
    "            tgt1_1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            src1_2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt1_2 = proxy_inds[target_inds_similar[ind,:]].to(self.device)\n",
    "            \n",
    "            src2_1 = proxy_inds[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            tgt2_1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            src2_2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt2_2 = proxy_inds[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            \n",
    "            src1 = torch.cat((src1_1, src1_2))\n",
    "            tgt1 = torch.cat((tgt1_1, tgt1_2))\n",
    "            src2 = torch.cat((src2_1, src2_2))\n",
    "            tgt2 = torch.cat((tgt2_1, tgt2_2))\n",
    "            \n",
    "            src_ggu2 = torch.cat((src1, src2))\n",
    "            tgt_ggu2 = torch.cat((tgt1, tgt2))\n",
    "                        \n",
    "            global_src = torch.cat((global_src, src_ggu2))\n",
    "            global_tgt = torch.cat((global_tgt, tgt_ggu2)) \n",
    "            \n",
    "            \n",
    "            src3_1 = proxy_inds_new[target_inds_similar[ind,:]].to(self.device)\n",
    "            tgt3_1 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            src3_2 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            tgt3_2 = proxy_inds_new[target_inds_similar[ind,:]].to(self.device)\n",
    "            \n",
    "            src4_1 = proxy_inds_new[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            tgt4_1 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            src4_2 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            tgt4_2 = proxy_inds_new[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            \n",
    "            src3 = torch.cat((src3_1*2+1, src3_2*2+1))\n",
    "            src3 = torch.cat((src3, torch.tensor([proxy_inds_new[ind]*2]).to(self.device)))\n",
    "            tgt3 = torch.cat((tgt3_1*2+1, tgt3_2*2+1))\n",
    "            tgt3 = torch.cat((tgt3, torch.tensor([proxy_inds_new[ind]*2+1]).to(self.device)))\n",
    "            src4 = torch.cat((src4_1*2+1, src4_2*2+1))\n",
    "            tgt4 = torch.cat((tgt4_1*2+1, tgt4_2*2+1))\n",
    "            \n",
    "            src_ggu2_pe = torch.cat((src3, src4))\n",
    "            tgt_ggu2_pe = torch.cat((tgt3, tgt4))\n",
    "\n",
    "            global_src_pos = torch.cat((global_src_pos, src_ggu2_pe))\n",
    "            global_tgt_pos = torch.cat((global_tgt_pos, tgt_ggu2_pe)) \n",
    "        \n",
    "\n",
    "        GGU_P1_g = dgl.graph((global_src.long(), global_tgt.long())).to(self.device)\n",
    "        GGU_P1_k = torch.ones(proxy_inds.shape[0]*num_global_neighbors*4, 1).to(self.device)\n",
    "#         GGU_P1_k = torch.ones(proxy_inds.shape[0]*num_global_neighbors*2, 1).to(self.device)\n",
    "        GGU_P1_k = nn.init.xavier_uniform_(GGU_P1_k)\n",
    "    \n",
    "    \n",
    "        GGU_P1_g_pos = dgl.graph((global_src_pos.long(), global_tgt_pos.long())).to(self.device)\n",
    "        pos_enc_ggu1 = self.positional_encoding(GGU_P1_g_pos, self.pos_enc_dim)\n",
    "        batch_pos_enc = torch.cat((batch_pos_enc.to(self.device), pos_enc_ggu1.to(self.device)), dim=0).to(self.device)\n",
    "\n",
    "        return h, SGU_g, GGU_P1_g, GGU_P2_g, SGU_k, GGU_P1_k, GGU_P2_k, batch_pos_enc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, graphs, batch_pos_enc=None):\n",
    "        original_length = graphs.num_nodes()\n",
    "        indices = []    \n",
    "        edge_lengths = []\n",
    "        node_lengths = []\n",
    "        for graph in dgl.unbatch(graphs):\n",
    "            adj = graph.adjacency_matrix().indices()\n",
    "            ind1 = adj[0]\n",
    "            ind2 = adj[1]\n",
    "            inds = [ind1, ind2]\n",
    "            edge_lengths.append(graph.num_edges())\n",
    "            node_lengths.append(graph.num_nodes())\n",
    "            indices.append(inds)\n",
    "        \n",
    "        h_p = self.embedding_h(graphs.ndata['feat'])\n",
    "        graphs.ndata['feat'] = h_p\n",
    "        h, SGU_g, GGU_P1_g, GGU_P2_g, SGU_k, GGU_P1_k, GGU_P2_k, batch_pos_enc = self.GDG(graphs, indices, edge_lengths, node_lengths, batch_pos_enc)\n",
    "\n",
    "        for ind, conv in enumerate(self.layers):\n",
    "            if ind < len(self.layers)-1:\n",
    "                SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k = conv(SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k, original_length, ind, batch_pos_enc)\n",
    "            else:\n",
    "                best_scores = conv(SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k, original_length, ind, batch_pos_enc)\n",
    "        return best_scores\n",
    "\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        V = label.size(0)\n",
    "        label_count = torch.bincount(label)\n",
    "        label_count = label_count[label_count.nonzero()].squeeze()\n",
    "        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)\n",
    "        cluster_sizes[torch.unique(label)] = label_count\n",
    "        weight = (V - cluster_sizes).float() / V\n",
    "        weight *= (cluster_sizes>0).float()\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_params = {}\n",
    "net_params['device'] = device\n",
    "net_params['in_dim_node'] = torch.unique(trainset[0][0].ndata['feat'],dim=0).size(0) # node_dim (feat is an integer)\n",
    "net_params['in_dim'] = 15     #10->9\n",
    "net_params['hidden_dim'] = 100\n",
    "net_params['out_dim'] = 100\n",
    "num_classes = torch.unique(trainset[0][1],dim=0).size(0)\n",
    "net_params['n_classes'] = num_classes\n",
    "net_params['L'] = 7  # min L should be 2\n",
    "net_params['readout'] = \"mean\"\n",
    "net_params['layer_norm'] = True\n",
    "net_params['batch_norm'] = True\n",
    "net_params['in_feat_dropout'] = 0.0\n",
    "net_params['dropout'] = 0.0\n",
    "net_params['residual'] = True\n",
    "net_params['edge_feat'] = False\n",
    "net_params['self_loop'] = False\n",
    "net_params['pos_enc'] = True\n",
    "net_params['pos_enc_dim'] = 4\n",
    "\n",
    "start0 = time.time()\n",
    "if net_params['pos_enc']:\n",
    "    print(\"[!] Adding graph positional encoding.\")\n",
    "    dataset._add_positional_encodings(net_params['pos_enc_dim'])\n",
    "    print('Time PE:',time.time()-start0)\n",
    "    \n",
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=dataset.collate, drop_last=True)\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=False, collate_fn=dataset.collate, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False, collate_fn=dataset.collate, drop_last=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "torch.cuda.manual_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIGNet(net_params, num_classes, 2, device)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=0)     \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True, min_lr=0.00001, mode='min')\n",
    "# scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=45, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network_sparse(model, device, data_loader, epoch):\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "            batch_graphs = batch_graphs.to(device)\n",
    "            batch_x = batch_graphs.ndata['feat'].to(device)\n",
    "            batch_e = batch_graphs.edata['feat'].to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            try:\n",
    "                batch_pos_enc = batch_graphs.ndata['pos_enc'].to(device)\n",
    "                batch_scores = model.forward(batch_graphs, batch_pos_enc)\n",
    "            except:\n",
    "                batch_scores = model.forward(batch_graphs)\n",
    "                \n",
    "            loss = model.loss(batch_scores, batch_labels) \n",
    "            epoch_test_loss += loss.detach().item()\n",
    "            epoch_test_acc += accuracy_SBM(batch_scores, batch_labels)\n",
    "\n",
    "        epoch_test_loss /= (iter + 1)\n",
    "        epoch_test_acc /= (iter + 1)\n",
    "    return epoch_test_loss, epoch_test_acc\n",
    "\n",
    "def train_epoch_sparse(model, optimizer, device, data_loader, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_train_acc = 0\n",
    "\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_graphs = batch_graphs.to(device)\n",
    "        batch_x = batch_graphs.ndata['feat'].to(device)\n",
    "        batch_e = batch_graphs.edata['feat'].to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            batch_pos_enc = batch_graphs.ndata['pos_enc'].to(device)\n",
    "            sign_flip = torch.rand(batch_pos_enc.size(1)).to(device)\n",
    "            sign_flip[sign_flip>=0.5] = 1.0; sign_flip[sign_flip<0.5] = -1.0\n",
    "            batch_pos_enc = batch_pos_enc * sign_flip.unsqueeze(0)\n",
    "            batch_scores = model.forward(batch_graphs, batch_pos_enc)\n",
    "        except:\n",
    "            batch_scores = model.forward(batch_graphs)\n",
    "        \n",
    "        \n",
    "        loss = model.loss(batch_scores, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_train_acc += accuracy_SBM(batch_scores, batch_labels)\n",
    "\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_train_acc /= (iter + 1)\n",
    "    return epoch_loss, epoch_train_acc, optimizer\n",
    "\n",
    "\n",
    "def accuracy_SBM(scores, targets):\n",
    "    S = targets.cpu().numpy()\n",
    "    C = np.argmax( torch.nn.Softmax(dim=1)(scores).cpu().detach().numpy() , axis=1 )\n",
    "    CM = confusion_matrix(S,C).astype(np.float32)\n",
    "    nb_classes = CM.shape[0]\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    nb_non_empty_classes = 0\n",
    "    pr_classes = np.zeros(nb_classes)\n",
    "    for r in range(nb_classes):\n",
    "        cluster = np.where(targets==r)[0]\n",
    "        if cluster.shape[0] != 0:\n",
    "            pr_classes[r] = CM[r,r]/ float(cluster.shape[0])\n",
    "            if CM[r,r]>0:\n",
    "                nb_non_empty_classes += 1\n",
    "        else:\n",
    "            pr_classes[r] = 0.0\n",
    "    acc = 100.* np.sum(pr_classes)/ float(nb_classes)\n",
    "    return acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = os.path.join('./tf-logs/', \"DATA_PATTERN\")    \n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "with tqdm(range(360)) as t:\n",
    "    for epoch in t:\n",
    "        t.set_description('Epoch %d' % epoch)\n",
    "        epoch_train_loss, epoch_train_acc, optimizer = train_epoch_sparse(model, optimizer, device, train_loader, epoch)\n",
    "        epoch_val_loss, epoch_val_acc = evaluate_network_sparse(model, device, val_loader, epoch)\n",
    "        _, epoch_test_acc = evaluate_network_sparse(model, device, test_loader, epoch)                \n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        writer.add_scalar('train/_loss', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('val/_loss', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('train/_acc', epoch_train_acc, epoch)\n",
    "        writer.add_scalar('val/_acc', epoch_val_acc, epoch)\n",
    "        writer.add_scalar('test/_acc', epoch_test_acc, epoch)\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        \n",
    "        ckpt_dir = os.path.join(os.getcwd(), \"MODEL_PATTERN\")\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        torch.save(model.state_dict(), '{}.pkl'.format(ckpt_dir + \"/epoch_\" + str(epoch)))\n",
    "\n",
    "        t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0]['lr'],\n",
    "                      train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n",
    "                      train_acc=epoch_train_acc, val_acc=epoch_val_acc, \n",
    "                      test_acc=epoch_test_acc)    \n",
    "\n",
    "        scheduler.step(epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(model, os.getcwd()+'/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
