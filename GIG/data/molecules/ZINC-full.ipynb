{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "from scipy import sparse as sp\n",
    "import optuna\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import warnings\n",
    "from dgl.data import ZINCDataset\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset, valset, testset = ZINCDataset(mode=\"train\"), ZINCDataset(mode=\"valid\"), ZINCDataset(mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../') \n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.superpixels import SuperPixDatasetDGL \n",
    "\n",
    "from data.data import LoadData\n",
    "from torch.utils.data import DataLoader\n",
    "from data.superpixels import SuperPixDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'ZINC'\n",
    "dataset = LoadData(DATASET_NAME) \n",
    "# trainset, valset, testset = dataset.train, dataset.val, dataset.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "from bisect import bisect_right\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "class LearnableDropout(nn.Module):\n",
    "    def __init__(self, initial_p=0.0, min_p=0.0, max_p=0.0001):\n",
    "        super(LearnableDropout, self).__init__()\n",
    "        self.p = nn.Parameter(torch.tensor(initial_p))\n",
    "        self.min_p = min_p\n",
    "        self.max_p = max_p\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = torch.clamp(self.p, self.min_p, self.max_p)\n",
    "        return F.dropout(x, p.item(), self.training)\n",
    "\n",
    "class MLPReadout(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, L=3): \n",
    "        super().__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim//2**l , input_dim//2**(l+1) , bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim//2**L , output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "\n",
    "    def act(self, x):\n",
    "        return x*(torch.tanh(F.softplus(x)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            # y = self.act(y)\n",
    "            # y = F.relu(y)\n",
    "            y = torch.nn.functional.silu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedGCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, batch_norm, residual=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_dim\n",
    "        self.out_channels = output_dim\n",
    "        self.dropout = dropout\n",
    "        # self.dropout = LearnableDropout()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        if input_dim != output_dim:\n",
    "            self.residual = False\n",
    "        \n",
    "        self.A = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.B = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.C = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.D = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.E = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "    \n",
    "    def act(self, x):\n",
    "        return x*(torch.tanh(F.softplus(x)))\n",
    "    \n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        Bh_j = edges.src['Bh']    \n",
    "        e_ij = edges.data['Ce'] +  edges.src['Dh'] + edges.dst['Eh'] \n",
    "        edges.data['e'] = e_ij\n",
    "        return {'Bh_j' : Bh_j, 'e_ij' : e_ij}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        Ah_i = nodes.data['Ah']\n",
    "        Bh_j = nodes.mailbox['Bh_j']\n",
    "        e = nodes.mailbox['e_ij'] \n",
    "        sigma_ij = torch.sigmoid(e) \n",
    "        h = Ah_i + torch.sum( sigma_ij * Bh_j, dim=1 ) / ( torch.sum( sigma_ij, dim=1 ) + 1e-6 )      \n",
    "        return {'h' : h}\n",
    "    \n",
    "    def forward(self, g, h, e):\n",
    "        h_in = h \n",
    "        e_in = e \n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) \n",
    "        g.ndata['Bh'] = self.B(h) \n",
    "        g.ndata['Dh'] = self.D(h)\n",
    "        g.ndata['Eh'] = self.E(h) \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e) \n",
    "        \n",
    "        g.apply_edges(fn.u_add_v('Dh', 'Eh', 'DEh'))\n",
    "        g.edata['e'] = g.edata['DEh'] + g.edata['Ce']\n",
    "        g.edata['sigma'] = torch.sigmoid(g.edata['e'])\n",
    "        g.update_all(fn.u_mul_e('Bh', 'sigma', 'm'), fn.sum('m', 'sum_sigma_h'))\n",
    "        g.update_all(fn.copy_e('sigma', 'm'), fn.sum('m', 'sum_sigma'))\n",
    "        g.ndata['h'] = g.ndata['Ah'] + g.ndata['sum_sigma_h'] / (g.ndata['sum_sigma'] + 1e-6)\n",
    "\n",
    "        h = g.ndata['h'] # result of graph convolution\n",
    "        e = g.edata['e'] # result of graph convolution\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            h = self.bn_node_h(h) # batch normalization  \n",
    "            e = self.bn_node_e(e) # batch normalization  \n",
    "        \n",
    "        # h = F.leaky_relu(h) # non-linear activation\n",
    "        # e = F.leaky_relu(e) # non-linear activation\n",
    "        h = self.act(h) # non-linear activation\n",
    "        e = self.act(e) # non-linear activation\n",
    "\n",
    "        \n",
    "        if self.residual:\n",
    "            h = h_in + h # residual connection\n",
    "            e = e_in + e # residual connection\n",
    "        \n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        e = F.dropout(e, self.dropout, training=self.training)\n",
    "        # h = self.dropout(h)\n",
    "        # e = self.dropout(e)\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}(in_channels={}, out_channels={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels)\n",
    "\n",
    "    \n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, net_params, update_type, embedding_e1, embedding_e2):\n",
    "        super().__init__()\n",
    "        in_dim = net_params['in_dim']\n",
    "        in_dim_edge = 1\n",
    "        hidden_dim = net_params['hidden_dim']\n",
    "        out_dim = net_params['out_dim']\n",
    "        dropout = net_params['dropout']\n",
    "        n_layers = net_params['L']\n",
    "        self.readout = net_params['readout']\n",
    "        self.batch_norm = net_params['batch_norm']\n",
    "        self.residual = net_params['residual']\n",
    "        self.edge_feat = net_params['edge_feat']\n",
    "        self.device = net_params['device']\n",
    "        self.update_type = update_type\n",
    "        self.pos_enc = net_params['pos_enc']\n",
    "        if self.pos_enc:\n",
    "            pos_enc_dim = net_params['pos_enc_dim']\n",
    "            self.embedding_pos_enc = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        # self.embedding_h1 = nn.Linear(in_dim, hidden_dim)\n",
    "        # self.embedding_h2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # self.embedding_e1 = embedding_e1\n",
    "        # self.embedding_e2 = embedding_e2\n",
    "        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n",
    "                                                    self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n",
    "        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.batch_norm, self.residual))\n",
    "\n",
    "        \n",
    "    def forward(self, g, h, e, stage, h_pos_enc=None):\n",
    "        # if stage == 0:\n",
    "        #     if self.update_type == 1:\n",
    "        #         # h = self.embedding_h1(h)\n",
    "        #         e = self.embedding_e1(e)\n",
    "        #     else:\n",
    "        #         # h = self.embedding_h2(h)\n",
    "        #         e = self.embedding_e2(e)\n",
    "        if self.pos_enc:\n",
    "            h_pos_enc = self.embedding_pos_enc(h_pos_enc.float()) \n",
    "            h = h + h_pos_enc \n",
    "        \n",
    "        for conv in self.layers:\n",
    "            h, e = conv(g, h, e)\n",
    "        return h, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGLayer(nn.Module):\n",
    "    def __init__(self, net_params, device, is_last_layer, SGU_layer_num, GGU1_layer_num, GGU2_layer_num, embedding_e1, embedding_e2):\n",
    "        super(GIGLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.is_last_layer = is_last_layer\n",
    "        SGU_params = net_params.copy()\n",
    "        GGU_P1_params = net_params.copy()\n",
    "        GGU_P2_params = net_params.copy()\n",
    "        SGU_params['L'] = SGU_layer_num\n",
    "        GGU_P1_params['L'] = GGU1_layer_num\n",
    "        GGU_P2_params['L'] = GGU2_layer_num\n",
    "        GGU_P1_params['in_dim'] = net_params['hidden_dim']\n",
    "        GGU_P2_params['in_dim'] = net_params['hidden_dim']\n",
    "        self.SGU = GNN(SGU_params, 1, embedding_e1, embedding_e2)\n",
    "        self.GGU_Part1 = GNN(GGU_P1_params, 2, embedding_e1, embedding_e2)\n",
    "        self.GGU_Part2 = GNN(GGU_P2_params, 2, embedding_e1, embedding_e2)\n",
    "        self.MLP_layer = MLPReadout(net_params['out_dim'], 1)\n",
    "\n",
    "    \n",
    "    def forward(self, g_ori, SGU_graph, GGU_P1_graph, GGU_P2_graph, h, SGU_e, GGU_P1_e, GGU_P2_e, original_length, stage, batch_pos_enc):\n",
    "        h1, e1 = self.SGU(SGU_graph, h, SGU_e, stage, batch_pos_enc)\n",
    "        h2, e2 = self.GGU_Part1(GGU_P1_graph, h1, GGU_P1_e, stage, batch_pos_enc)\n",
    "        h3, e3 = self.GGU_Part2(GGU_P2_graph, h2, GGU_P2_e, stage, batch_pos_enc)\n",
    "\n",
    "\n",
    "        if not self.is_last_layer:\n",
    "            return SGU_graph, GGU_P1_graph, GGU_P2_graph, h3, e1, e2, e3\n",
    "        else:\n",
    "            h_r = h3[:original_length, :]\n",
    "            g_ori.ndata['h'] = h_r\n",
    "            hg = dgl.mean_nodes(g_ori, 'h')\n",
    "            return self.MLP_layer(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dimension_GNN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(Dimension_GNN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_feats)\n",
    "        self.conv2 = GraphConv(hidden_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "class GIGNet(nn.Module):\n",
    "    def __init__(self, net_params, n_layers, device, SGU_layer_num, GGU1_layer_num, GGU2_layer_num):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_e1 = nn.Linear(1, net_params[\"hidden_dim\"])\n",
    "        self.embedding_e2 = nn.Linear(1, net_params[\"hidden_dim\"])\n",
    "        \n",
    "        self.layers = nn.ModuleList([GIGLayer(net_params, device, False, SGU_layer_num, GGU1_layer_num, GGU2_layer_num, self.embedding_e1, self.embedding_e2) for _ in range(n_layers-1)])\n",
    "        self.layers.append(GIGLayer(net_params, device, True, SGU_layer_num, GGU1_layer_num, GGU2_layer_num, self.embedding_e1, self.embedding_e2))\n",
    "        self.embedding_h = nn.Embedding(net_params['num_atom_type'], net_params['in_dim'])\n",
    "        self.embedding_e = nn.Embedding(net_params['num_atom_type'], net_params['in_dim'])\n",
    "        self.vertex_linear = nn.Linear(net_params['in_dim'], net_params['hidden_dim'])\n",
    "        self.edge_linear = nn.Linear(net_params['in_dim'], net_params['hidden_dim'])\n",
    "        # self.mean_linear = Dimension_GNN(net_params['in_dim'], 50, net_params['hidden_dim'])\n",
    "        self.in_dim = net_params['in_dim']\n",
    "        self.hidden_dim = net_params['hidden_dim']\n",
    "        self.pos_enc_dim = net_params['pos_enc_dim']\n",
    "\n",
    "    def positional_encoding(self, g, pos_enc_dim):\n",
    "        # A = g.adjacency_matrix_scipy(return_edge_ids=False).astype(float)\n",
    "        A = g.adj_external().to_dense()\n",
    "        N = sp.diags(dgl.backend.asnumpy(g.in_degrees()).clip(1) ** -0.5, dtype=float)\n",
    "        L = sp.eye(g.number_of_nodes()) - N * A * N\n",
    "        \n",
    "        EigVal, EigVec = np.linalg.eig(L)\n",
    "        idx = EigVal.argsort() # increasing order\n",
    "        EigVal, EigVec = EigVal[idx], np.real(EigVec[:,idx])\n",
    "        return torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).to(self.device).float() \n",
    "\n",
    "    def proxy_edge_construction(self, proxy_vertex, sub_vertex, num_proxy_edges):\n",
    "        sub_vertex = sub_vertex.permute(1,0).to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', proxy_vertex, sub_vertex).to(self.device)\n",
    "        diag = torch.diag(si).to(self.device)\n",
    "        a_diag = torch.diag_embed(diag).to(self.device)\n",
    "        si = (si - a_diag).to(self.device)\n",
    "        _, target_inds_nonsim = si.topk(k=num_proxy_edges, dim=1, largest=True)\n",
    "        _, target_inds_sim = si.topk(k=num_proxy_edges, dim=1, largest=False)\n",
    "        return target_inds_nonsim.squeeze(0).to(self.device), target_inds_sim.squeeze(0).to(self.device)\n",
    "\n",
    "    def global_neighbors(self, h, num_global_neighbors):\n",
    "        h = h.to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', h, h.transpose(0, 1)).to(self.device)\n",
    "        diag = torch.diag(si).to(self.device)\n",
    "        a_diag = torch.diag_embed(diag).to(self.device)\n",
    "        si = (si - a_diag).to(self.device)\n",
    "        _, target_inds_nonsim = si.topk(k=num_global_neighbors, dim=1, largest=True)\n",
    "        _, target_inds_sim = si.topk(k=num_global_neighbors, dim=1, largest=False)\n",
    "        return target_inds_sim.to(self.device), target_inds_nonsim.to(self.device)\n",
    "    \n",
    "    \n",
    "    def GDG(self, graphs, indices, edge_lengths, node_lengths, batch_pos_enc):\n",
    "        graphs.ndata['feat'] = self.vertex_linear(graphs.ndata['feat'])\n",
    "        graphs.edata['feat'] = self.edge_linear(graphs.edata['feat'])\n",
    "        \n",
    "        global_src = torch.zeros((0)).to(self.device)\n",
    "        global_tgt = torch.zeros((0)).to(self.device)\n",
    "        global_src_pos = torch.zeros((0)).to(self.device)\n",
    "        global_tgt_pos = torch.zeros((0)).to(self.device)\n",
    "        src1 = torch.zeros((0)).to(self.device)\n",
    "        tgt1 = torch.zeros((0)).to(self.device)\n",
    "        src3 = torch.zeros((0)).to(self.device)\n",
    "        tgt3 = torch.zeros((0)).to(self.device)\n",
    "        node_index = 0\n",
    "        current_ind = graphs.ndata['feat'].shape[0]\n",
    "        proxy_inds = []\n",
    "        proxy_vertices = torch.zeros((0)).to(self.device)\n",
    "        proxy_vertices_gi_lo = torch.zeros((0)).to(self.device)\n",
    "        proxy_edge_counter = 0\n",
    "        for ind, indice in enumerate(indices):\n",
    "            node_length = node_lengths[ind]\n",
    "            edge_length = edge_lengths[ind]\n",
    "            \n",
    "            h = graphs.ndata['feat'][node_index:node_index+node_length, :].to(self.device)\n",
    "            \n",
    "            proxy_vertex = h.mean(dim=0).reshape(1, self.hidden_dim).to(self.device)\n",
    "            proxy_vertex_li_go = proxy_vertex.clone()\n",
    "            proxy_vertex_gi_lo = torch.ones(1, self.hidden_dim).to(self.device)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex_li_go), dim=0)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex_gi_lo), dim=0)\n",
    "            proxy_vertices_gi_lo = torch.cat((proxy_vertices_gi_lo, proxy_vertex_gi_lo), dim=0)\n",
    "            \n",
    "            proxy_node_length = int(node_length * 0.1)\n",
    "            proxy_edge_counter += proxy_node_length\n",
    "            \n",
    "            \n",
    "            src1_sub, tgt3_sub = self.proxy_edge_construction(proxy_vertex, h, proxy_node_length)\n",
    "            src1_sub = src1_sub + node_index\n",
    "            tgt3_sub = tgt3_sub + node_index\n",
    "            tgt1_sub = torch.full([proxy_node_length,], current_ind).to(self.device)   \n",
    "            \n",
    "            src3_sub = torch.cat((torch.full([proxy_node_length,], current_ind+1).to(self.device), torch.tensor([current_ind]).to(self.device)))\n",
    "            tgt3_sub = torch.cat((tgt3_sub, torch.tensor([current_ind+1]).to(self.device)))\n",
    "            # src3_sub = torch.full([proxy_node_length,], current_ind+1).to(self.device)\n",
    "\n",
    "            \n",
    "            src1 = torch.cat((src1, src1_sub))\n",
    "            tgt1 = torch.cat((tgt1, tgt1_sub))\n",
    "            src3 = torch.cat((src3, src3_sub))\n",
    "            tgt3 = torch.cat((tgt3, tgt3_sub))\n",
    "            \n",
    "            \n",
    "            node_index += node_length\n",
    "            proxy_inds.append(current_ind+1)\n",
    "            current_ind += 2\n",
    "        \n",
    "        src1 = torch.cat((graphs.adj().indices()[0].to(self.device), src1))\n",
    "        tgt1 = torch.cat((graphs.adj().indices()[1].to(self.device), tgt1))\n",
    "        SGU_g = dgl.graph((src1.long(), tgt1.long())).to(self.device)\n",
    "        \n",
    "        SGU_g.add_nodes(1)\n",
    "        GGU_P2_g = dgl.graph((src3.long(), tgt3.long())).to(self.device)        \n",
    "        h = torch.cat((graphs.ndata['feat'], proxy_vertices))\n",
    "\n",
    "        SGU_iso = h.shape[0] - SGU_g.num_nodes()\n",
    "        GGU_P2_g_iso = h.shape[0] - GGU_P2_g.num_nodes()\n",
    "        \n",
    "        if SGU_iso != 0:\n",
    "            SGU_g.add_nodes(SGU_iso)\n",
    "        if GGU_P2_g_iso != 0:\n",
    "            GGU_P2_g.add_nodes(GGU_P2_g_iso)\n",
    "        \n",
    "        SGU_k = torch.cat((graphs.edata['feat'], self.embedding_e1(torch.randn(proxy_edge_counter, 1).to(self.device)).to(self.device)))\n",
    "        GGU_P2_k = self.embedding_e2(torch.ones(proxy_edge_counter+len(indices), 1).to(self.device)).to(self.device)\n",
    "        # GGU_P2_k = torch.randn(proxy_edge_counter, 1).to(self.device)\n",
    "\n",
    "        \n",
    "        num_global_neighbors = 9\n",
    "        proxy_inds = torch.tensor(proxy_inds).to(self.device)     \n",
    "        proxy_inds_new = torch.arange(len(node_lengths)*2).to(self.device) \n",
    "        num_global_neighbors = int(num_global_neighbors/2)\n",
    "        target_inds_similar, target_inds_nonsimilar = self.global_neighbors(proxy_vertices_gi_lo, num_global_neighbors)\n",
    " \n",
    "\n",
    "        for ind, i in enumerate(proxy_inds):\n",
    "            src1_1 = proxy_inds[target_inds_similar[ind,:]].to(self.device)\n",
    "            tgt1_1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            src1_2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt1_2 = proxy_inds[target_inds_similar[ind,:]].to(self.device)\n",
    "            \n",
    "            src2_1 = proxy_inds[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            tgt2_1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            src2_2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt2_2 = proxy_inds[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            \n",
    "            src1 = torch.cat((src1_1, src1_2))\n",
    "            tgt1 = torch.cat((tgt1_1, tgt1_2))\n",
    "            src2 = torch.cat((src2_1, src2_2))\n",
    "            tgt2 = torch.cat((tgt2_1, tgt2_2))\n",
    "            \n",
    "            src_ggu2 = torch.cat((src1, src2))\n",
    "            tgt_ggu2 = torch.cat((tgt1, tgt2))\n",
    "            src_ggu2 = torch.cat((src_ggu2, torch.tensor([i-1]).to(self.device)))\n",
    "            tgt_ggu2 = torch.cat((tgt_ggu2, torch.tensor([i]).to(self.device)))\n",
    "                        \n",
    "            global_src = torch.cat((global_src, src_ggu2))\n",
    "            global_tgt = torch.cat((global_tgt, tgt_ggu2)) \n",
    "            \n",
    "            \n",
    "            src3_1 = proxy_inds_new[target_inds_similar[ind,:]].to(self.device)\n",
    "            tgt3_1 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            src3_2 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            tgt3_2 = proxy_inds_new[target_inds_similar[ind,:]].to(self.device)\n",
    "            \n",
    "            src4_1 = proxy_inds_new[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            tgt4_1 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            src4_2 = torch.full([num_global_neighbors,], proxy_inds_new[ind]).to(self.device)\n",
    "            tgt4_2 = proxy_inds_new[target_inds_nonsimilar[ind,:]].to(self.device)\n",
    "            \n",
    "            src3 = torch.cat((src3_1*2+1, src3_2*2+1))\n",
    "            src3 = torch.cat((src3, torch.tensor([proxy_inds_new[ind]*2]).to(self.device)))\n",
    "            tgt3 = torch.cat((tgt3_1*2+1, tgt3_2*2+1))\n",
    "            tgt3 = torch.cat((tgt3, torch.tensor([proxy_inds_new[ind]*2+1]).to(self.device)))\n",
    "            src4 = torch.cat((src4_1*2+1, src4_2*2+1))\n",
    "            tgt4 = torch.cat((tgt4_1*2+1, tgt4_2*2+1))\n",
    "            \n",
    "            src_ggu2_pe = torch.cat((src3, src4))\n",
    "            tgt_ggu2_pe = torch.cat((tgt3, tgt4))\n",
    "\n",
    "            global_src_pos = torch.cat((global_src_pos, src_ggu2_pe))\n",
    "            global_tgt_pos = torch.cat((global_tgt_pos, tgt_ggu2_pe)) \n",
    "        \n",
    "\n",
    "        GGU_P1_g = dgl.graph((global_src.long(), global_tgt.long())).to(self.device)\n",
    "        # GGU_P1_k = self.embedding_e1(torch.randn(proxy_inds.shape[0]*num_global_neighbors*4, 1).to(self.device)).to(self.device)\n",
    "        GGU_P1_k = self.embedding_e1(torch.ones(proxy_inds.shape[0]*num_global_neighbors*4+len(proxy_inds), 1).to(self.device)).to(self.device)\n",
    "#         GGU_P1_k = torch.ones(proxy_inds.shape[0]*num_global_neighbors*2, 1).to(self.device)\n",
    "    \n",
    "        GGU_P1_g_pos = dgl.graph((global_src_pos.long(), global_tgt_pos.long())).to(self.device)\n",
    "        pos_enc_ggu1 = self.positional_encoding(GGU_P1_g_pos, self.pos_enc_dim)\n",
    "        if batch_pos_enc != None:\n",
    "            batch_pos_enc = torch.cat((batch_pos_enc.to(self.device), pos_enc_ggu1.to(self.device)), dim=0).to(self.device)\n",
    "        return h, SGU_g, GGU_P1_g, GGU_P2_g, SGU_k, GGU_P1_k, GGU_P2_k, batch_pos_enc\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, graphs, batch_pos_enc=None):\n",
    "        original_length = graphs.num_nodes()\n",
    "        indices = []    \n",
    "        edge_lengths = []\n",
    "        node_lengths = []\n",
    "        for graph in dgl.unbatch(graphs):\n",
    "            adj = graph.adjacency_matrix().indices()\n",
    "            ind1 = adj[0]\n",
    "            ind2 = adj[1]\n",
    "            inds = [ind1, ind2]\n",
    "            edge_lengths.append(graph.num_edges())\n",
    "            node_lengths.append(graph.num_nodes())\n",
    "            indices.append(inds)\n",
    "        \n",
    "        h_p = self.embedding_h(graphs.ndata['feat'])\n",
    "        graphs.ndata['feat'] = h_p\n",
    "        e_p = self.embedding_e(graphs.edata['feat'])\n",
    "        graphs.edata['feat'] = e_p\n",
    "        h, SGU_g, GGU_P1_g, GGU_P2_g, SGU_k, GGU_P1_k, GGU_P2_k, batch_pos_enc = self.GDG(graphs, indices, edge_lengths, node_lengths, batch_pos_enc)\n",
    "\n",
    "        for ind, conv in enumerate(self.layers):\n",
    "            if ind < len(self.layers)-1:\n",
    "                SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k = conv(graphs, SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k, original_length, ind, batch_pos_enc)\n",
    "            else:\n",
    "                best_scores = conv(graphs, SGU_g, GGU_P1_g, GGU_P2_g, h, SGU_k, GGU_P1_k, GGU_P2_k, original_length, ind, batch_pos_enc)\n",
    "        return best_scores\n",
    "\n",
    "    \n",
    "    def loss(self, scores, targets):\n",
    "        loss = nn.L1Loss()(scores, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "class WarmupCosineLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.after_warmup = total_epochs - warmup_epochs\n",
    "        super(WarmupCosineLR, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [base_lr * (self.last_epoch / self.warmup_epochs) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            cosine_epoch = self.last_epoch - self.warmup_epochs\n",
    "            return [base_lr * (1 + torch.cos(torch.tensor(cosine_epoch) / self.after_warmup * torch.pi)) / 2\n",
    "                    for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input samples is a list of pairs (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)       \n",
    "    \n",
    "    return batched_graph, labels\n",
    "\n",
    "\n",
    "def _add_positional_encodings(train, val, test, pos_enc_dim):\n",
    "    # Graph positional encoding v/ Laplacian eigenvectors\n",
    "    train.graph_lists = [positional_encoding(g, pos_enc_dim) for g in train.graph_lists]\n",
    "    val.graph_lists = [positional_encoding(g, pos_enc_dim) for g in val.graph_lists]\n",
    "    test.graph_lists = [positional_encoding(g, pos_enc_dim) for g in test.graph_lists]\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_generate(input_dim, hidden_dim):\n",
    "    net_params = {}\n",
    "    net_params['device'] = device\n",
    "    net_params['num_atom_type'] = dataset.num_atom_type\n",
    "    net_params['num_bond_type'] = dataset.num_bond_type\n",
    "    net_params['in_dim'] = input_dim \n",
    "    net_params['residual'] = True\n",
    "    net_params['hidden_dim'] = hidden_dim \n",
    "    net_params['out_dim'] = hidden_dim\n",
    "    net_params['n_heads'] = -1\n",
    "    net_params['L'] = 8  \n",
    "    net_params['readout'] = \"mean\"\n",
    "    net_params['layer_norm'] = True\n",
    "    net_params['batch_norm'] = True\n",
    "    net_params['in_feat_dropout'] = 0.0\n",
    "    net_params['dropout'] = 0.0\n",
    "    net_params['edge_feat'] = False\n",
    "    net_params['self_loop'] = False\n",
    "    net_params['pos_enc'] = True\n",
    "    net_params['pos_enc_dim'] = 4\n",
    "    return net_params\n",
    "\n",
    "net_params = params_generate(9, 105)\n",
    "start0 = time.time()\n",
    "if net_params['pos_enc']:\n",
    "    print(\"[!] Adding graph positional encoding.\")\n",
    "    dataset._add_positional_encodings(net_params['pos_enc_dim'])\n",
    "    print('Time PE:',time.time()-start0)\n",
    "    \n",
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test\n",
    "\n",
    "\n",
    "model = GIGNet(net_params, 1, device, 10, 6, 1).to(device)\n",
    "# bn_params = [param for name, param in model.named_parameters() if 'bn' in name]\n",
    "# other_params = [param for name, param in model.named_parameters() if 'bn' not in name]\n",
    "# model.apply(weights_init)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0012, weight_decay=0)\n",
    "# optimizer = optim.AdamW([{'params': bn_params, 'weight_decay': 0}, {'params': other_params, 'weight_decay': 1.1e-5}], lr=0.00136)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=30, T_mult=1, verbose=True)\n",
    "# scheduler = WarmupCosineLR(optimizer, 60, 1800)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, collate_fn=dataset.collate, drop_last=True)\n",
    "val_loader = DataLoader(valset, batch_size=128, shuffle=False, collate_fn=dataset.collate, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, collate_fn=dataset.collate, drop_last=True)\n",
    "# train_loader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate, drop_last=True)\n",
    "# val_loader = DataLoader(valset, batch_size=32, shuffle=False, collate_fn=collate, drop_last=True)\n",
    "# test_loader = DataLoader(testset, batch_size=32, shuffle=False, collate_fn=collate, drop_last=True)\n",
    "torch.cuda.manual_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_sparse(model, optimizer, device, data_loader, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_train_mae = 0\n",
    "    nb_data = 0\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_targets) in enumerate(data_loader):\n",
    "        batch_graphs = batch_graphs.to(device)\n",
    "        batch_x = batch_graphs.ndata['feat'].to(device)  # num x feat\n",
    "        batch_e = batch_graphs.edata['feat'].to(device)\n",
    "        batch_e = batch_e.reshape(batch_e.shape[0],1).float()\n",
    "        batch_targets = torch.tensor(batch_targets).to(device)\n",
    "        batch_targets = batch_targets.reshape(batch_targets.shape[0],1)\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            batch_pos_enc = batch_graphs.ndata['pos_enc'].to(device)\n",
    "            sign_flip = torch.rand(batch_pos_enc.size(1)).to(device)\n",
    "            sign_flip[sign_flip>=0.5] = 1.0; sign_flip[sign_flip<0.5] = -1.0\n",
    "            batch_pos_enc = batch_pos_enc * sign_flip.unsqueeze(0)\n",
    "            batch_scores = model.forward(batch_graphs, batch_pos_enc)\n",
    "        except:\n",
    "            batch_scores = model.forward(batch_graphs)\n",
    "        loss = model.loss(batch_scores, batch_targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_train_mae += MAE(batch_scores, batch_targets)\n",
    "        nb_data += batch_targets.size(0)          \n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_train_mae /= (iter + 1)\n",
    "    \n",
    "    return epoch_loss, epoch_train_mae, optimizer\n",
    "\n",
    "def evaluate_network_sparse(model, device, data_loader, epoch):\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_mae = 0\n",
    "    nb_data = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, (batch_graphs, batch_targets) in enumerate(data_loader):\n",
    "            batch_graphs = batch_graphs.to(device)\n",
    "            batch_x = batch_graphs.ndata['feat'].to(device)\n",
    "            batch_e = batch_graphs.edata['feat'].to(device)\n",
    "            batch_e = batch_e.reshape(batch_e.shape[0],1).float()\n",
    "            batch_targets = torch.tensor(batch_targets).to(device)\n",
    "            batch_targets = batch_targets.reshape(batch_targets.shape[0],1)\n",
    "            try:\n",
    "                batch_pos_enc = batch_graphs.ndata['pos_enc'].to(device)\n",
    "                batch_scores = model.forward(batch_graphs, batch_pos_enc)\n",
    "            except:\n",
    "                batch_scores = model.forward(batch_graphs)\n",
    "            loss = model.loss(batch_scores, batch_targets)\n",
    "            epoch_test_loss += loss.detach().item()\n",
    "            epoch_test_mae += MAE(batch_scores, batch_targets)\n",
    "            nb_data += batch_targets.size(0)\n",
    "        epoch_test_loss /= (iter + 1)\n",
    "        epoch_test_mae /= (iter + 1)\n",
    "        \n",
    "    return epoch_test_loss, epoch_test_mae\n",
    "\n",
    "\n",
    "def MAE(scores, targets):\n",
    "    MAE = F.l1_loss(scores, targets)\n",
    "    MAE = MAE.detach().item()\n",
    "    return MAE  \n",
    "\n",
    "def warmup_lr(epoch, base_lr, max_lr, warmup_epochs):\n",
    "    return ((max_lr - base_lr) / warmup_epochs) * epoch + base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('./tf-logs/', \"DATA_ZINC\")    \n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "base_lr = 0.0001  # 初始学习率\n",
    "max_lr = 0.0012   # warmup 结束时的学习率\n",
    "warmup_epochs = 60\n",
    "with tqdm.tqdm(range(1500), ncols=250) as t:\n",
    "    for epoch in t:\n",
    "        t.set_description('Epoch %d' % epoch)\n",
    "        epoch_train_loss, epoch_train_mae, optimizer = train_epoch_sparse(model, optimizer, device, train_loader, epoch)\n",
    "        epoch_val_loss, epoch_val_mae = evaluate_network_sparse(model, device, val_loader, epoch)\n",
    "        _, epoch_test_mae = evaluate_network_sparse(model, device, test_loader, epoch)                \n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        writer.add_scalar('train/_loss', epoch_train_loss, epoch)\n",
    "        writer.add_scalar('val/_loss', epoch_val_loss, epoch)\n",
    "        writer.add_scalar('train/_mae', epoch_train_mae, epoch)\n",
    "        writer.add_scalar('val/_mae', epoch_val_mae, epoch)\n",
    "        writer.add_scalar('test/_mae', epoch_test_mae, epoch)\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        \n",
    "        ckpt_dir = os.path.join(os.getcwd(), \"MODEL_ZINC\")\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        torch.save(model.state_dict(), '{}.pkl'.format(ckpt_dir + \"/epoch_\" + str(epoch)))\n",
    "   \n",
    "        t.set_postfix({'time':time.time()-start, 'lr':optimizer.param_groups[0]['lr'],\n",
    "                      'train_loss':epoch_train_loss, 'val_loss':epoch_val_loss,\n",
    "                      'train_mae':epoch_train_mae, 'val_mae':epoch_val_mae, \n",
    "                      'test_mae':epoch_test_mae})    \n",
    "        if epoch < warmup_epochs:\n",
    "            lr = warmup_lr(epoch, base_lr, max_lr, warmup_epochs)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            scheduler.step(epoch - warmup_epochs)\n",
    "\n",
    "        # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
