{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import glob\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../') # go to root folder of the project\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.superpixels import SuperPixDatasetDGL \n",
    "\n",
    "from data.data import LoadData\n",
    "from torch.utils.data import DataLoader\n",
    "from data.superpixels import SuperPixDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'ENZYMES'\n",
    "dataset = LoadData(DATASET_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import dgl\n",
    "import torch.optim as optim\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "class MLPReadout(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, L=2): \n",
    "        super().__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim//2**l , input_dim//2**(l+1) , bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim//2**L , output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = F.relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedGCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, batch_norm, residual=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_dim\n",
    "        self.out_channels = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        if input_dim != output_dim:\n",
    "            self.residual = False\n",
    "        \n",
    "        self.A = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.B = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.C = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.D = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.E = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        Bh_j = edges.src['Bh']    \n",
    "        e_ij = edges.data['Ce'] +  edges.src['Dh'] + edges.dst['Eh'] # e_ij = Ce_ij + Dhi + Ehj\n",
    "        edges.data['e'] = e_ij\n",
    "        return {'Bh_j' : Bh_j, 'e_ij' : e_ij}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        Ah_i = nodes.data['Ah']\n",
    "        Bh_j = nodes.mailbox['Bh_j']\n",
    "        e = nodes.mailbox['e_ij'] \n",
    "        sigma_ij = torch.sigmoid(e) # sigma_ij = sigmoid(e_ij)\n",
    "        h = Ah_i + torch.sum( sigma_ij * Bh_j, dim=1 ) / ( torch.sum( sigma_ij, dim=1 ) + 1e-6 )  # hi = Ahi + sum_j eta_ij/sum_j' eta_ij' * Bhj <= dense attention       \n",
    "        return {'h' : h}\n",
    "    \n",
    "    def forward(self, g, h, e):\n",
    "        h_in = h # for residual connection\n",
    "        e_in = e # for residual connection\n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) \n",
    "        g.ndata['Bh'] = self.B(h) \n",
    "        g.ndata['Dh'] = self.D(h)\n",
    "        g.ndata['Eh'] = self.E(h) \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e) \n",
    "        \n",
    "        g.apply_edges(fn.u_add_v('Dh', 'Eh', 'DEh'))\n",
    "        g.edata['e'] = g.edata['DEh'] + g.edata['Ce']\n",
    "        g.edata['sigma'] = torch.sigmoid(g.edata['e'])\n",
    "        g.update_all(fn.u_mul_e('Bh', 'sigma', 'm'), fn.sum('m', 'sum_sigma_h'))\n",
    "        g.update_all(fn.copy_e('sigma', 'm'), fn.sum('m', 'sum_sigma'))\n",
    "        g.ndata['h'] = g.ndata['Ah'] + g.ndata['sum_sigma_h'] / (g.ndata['sum_sigma'] + 1e-6)\n",
    "\n",
    "        h = g.ndata['h'] # result of graph convolution\n",
    "        e = g.edata['e'] # result of graph convolution\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            h = self.bn_node_h(h) # batch normalization  \n",
    "            e = self.bn_node_e(e) # batch normalization  \n",
    "        \n",
    "        \n",
    "        h = F.leaky_relu(h) # non-linear activation\n",
    "        e = F.leaky_relu(e) # non-linear activation\n",
    "        \n",
    "        if self.residual:\n",
    "            h = h_in + h # residual connection\n",
    "            e = e_in + e # residual connection\n",
    "        \n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        e = F.dropout(e, self.dropout, training=self.training)\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}(in_channels={}, out_channels={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels)\n",
    "\n",
    "    \n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, net_params, update_type):\n",
    "        super().__init__()\n",
    "        in_dim = net_params['in_dim']\n",
    "        in_dim_edge = net_params['in_dim_edge']\n",
    "        hidden_dim = net_params['hidden_dim']\n",
    "        out_dim = net_params['out_dim']\n",
    "        n_classes = net_params['n_classes']\n",
    "        dropout = net_params['dropout']\n",
    "        n_layers = net_params['L']\n",
    "        self.readout = net_params['readout']\n",
    "        self.batch_norm = net_params['batch_norm']\n",
    "        self.residual = net_params['residual']\n",
    "        self.edge_feat = net_params['edge_feat']\n",
    "        self.device = net_params['device']\n",
    "        self.update_type = update_type\n",
    "        \n",
    "        \n",
    "        self.embedding_h1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.embedding_h2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.embedding_e1 = nn.Linear(in_dim_edge, hidden_dim)\n",
    "        self.embedding_e2 = nn.Linear(in_dim_edge, hidden_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([ GatedGCNLayer(hidden_dim, hidden_dim, dropout,\n",
    "                                                    self.batch_norm, self.residual) for _ in range(n_layers-1) ]) \n",
    "        self.layers.append(GatedGCNLayer(hidden_dim, out_dim, dropout, self.batch_norm, self.residual))\n",
    "\n",
    "    def forward(self, g, h, e, stage):\n",
    "        if stage == 0:\n",
    "            if self.update_type == 1:\n",
    "                h = self.embedding_h1(h)\n",
    "                e = self.embedding_e1(e)\n",
    "            else:\n",
    "                h = self.embedding_h2(h)\n",
    "                e = self.embedding_e2(e)\n",
    "\n",
    "        for conv in self.layers:\n",
    "            h, e = conv(g, h, e)\n",
    "        return h, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGLayer(nn.Module):\n",
    "    def __init__(self, net_params, device, is_last_layer):\n",
    "        super(GIGLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.is_last_layer = is_last_layer\n",
    "        SGU_params = net_params.copy()\n",
    "        GGU_P1_params = net_params.copy()\n",
    "        GGU_P2_params = net_params.copy()\n",
    "        SGU_params['L'] = 2\n",
    "        GGU_P1_params['L'] = 1\n",
    "        GGU_P2_params['L'] = 1\n",
    "        GGU_P1_params['in_dim'] = net_params['hidden_dim']\n",
    "        GGU_P2_params['in_dim'] = net_params['hidden_dim']\n",
    "        self.SGU = GNN(SGU_params, 1)\n",
    "        self.GGU_Part1 = GNN(GGU_P1_params, 2)\n",
    "        self.GGU_Part2 = GNN(GGU_P2_params, 2)\n",
    "    \n",
    "    def forward(self, SGU_graph, GGU_P1_graph, GGU_P2_graph, h, SGU_e, GGU_P1_e, GGU_P2_e, stage):\n",
    "        h1, e1 = self.SGU(SGU_graph, h, SGU_e, stage)\n",
    "        h2, e2 = self.GGU_Part1(GGU_P1_graph, h1, GGU_P1_e, stage)\n",
    "        h3, e3 = self.GGU_Part2(GGU_P2_graph, h2, GGU_P2_e, stage)\n",
    "\n",
    "        if not self.is_last_layer:\n",
    "            return SGU_graph, GGU_P1_graph, GGU_P2_graph, h3, e1, e2, e3\n",
    "        else:\n",
    "            SGU_graph.ndata['h'] = h3\n",
    "            hg = dgl.mean_nodes(SGU_graph, 'h')\n",
    "            return hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIGNet(nn.Module):\n",
    "    def __init__(self, net_params, n_layers, num_neighbors, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.layers = nn.ModuleList([GIGLayer(net_params, device, False) for _ in range(n_layers-1)])\n",
    "        self.layers.append(GIGLayer(net_params, device, True))\n",
    "        self.MLP_layer = MLPReadout(net_params['out_dim'], net_params['n_classes'])\n",
    "\n",
    "    #Obtain global neighbors\n",
    "    def global_neighbors(self, h, num_global_neighbors):\n",
    "        h = h.to(self.device)\n",
    "        si = torch.einsum('i j , j k -> i k', h, h.transpose(0, 1)).to(self.device)\n",
    "        diag = torch.diag(si).to(self.device)\n",
    "        a_diag = torch.diag_embed(diag).to(self.device)\n",
    "        si = (si - a_diag).to(self.device)\n",
    "        _, target_inds = si.topk(k=num_global_neighbors, dim=1, largest=True)\n",
    "        return target_inds.to(self.device)\n",
    "    \n",
    "    #GIG data generation module\n",
    "    def GDG(self, graphs, indices, edge_lengths, node_lengths):\n",
    "        global_src = torch.zeros((0)).to(self.device)\n",
    "        global_tgt = torch.zeros((0)).to(self.device)\n",
    "        node_index = 0\n",
    "        edge_index = 0\n",
    "        SGU_graphs = []\n",
    "        GGU_P2_graphs = []\n",
    "        proxy_inds = []\n",
    "        proxy_vertices = torch.zeros((0)).to(self.device)\n",
    "        #graph construction in DGL for SGU and GGU part 2\n",
    "        for ind, indice in enumerate(indices):\n",
    "            node_length = node_lengths[ind]\n",
    "            edge_length = edge_lengths[ind]\n",
    "            \n",
    "            #Vertex and edge features from current sub-graph\n",
    "            h = graphs.ndata['feat'][node_index:node_index+node_length, :].to(self.device)\n",
    "            e = graphs.edata['feat'][edge_index:edge_index+edge_length, :].to(self.device)\n",
    "            \n",
    "            #Source and target for SGU module\n",
    "            src1 = torch.arange(node_length).to(self.device)\n",
    "            tgt1 = torch.full([node_length,], node_length).to(self.device)\n",
    "            src1 = torch.cat((indice[0].to(self.device), src1))\n",
    "            tgt1 = torch.cat((indice[1].to(self.device), tgt1))\n",
    "            \n",
    "            #Source and target for GGU part 2\n",
    "            src3 = torch.full([node_length,], node_length).to(self.device)\n",
    "            tgt3 = torch.arange(node_length).to(self.device)\n",
    "            \n",
    "            #SGU graph\n",
    "            SGU_g = dgl.graph((src1, tgt1)).to(self.device)\n",
    "            \n",
    "            #GGU part 2 graph\n",
    "            GGU_P2_g = dgl.graph((src3, tgt3)).to(self.device)\n",
    "            \n",
    "            #Proxy vertex feature initilization \n",
    "            proxy_vertex = h.mean(dim=0).reshape(1,18).to(self.device)\n",
    "            proxy_vertices = torch.cat((proxy_vertices, proxy_vertex), dim=0)\n",
    "            \n",
    "            #Vertex and edge features construction\n",
    "            SGU_node_features = torch.cat((h, proxy_vertex), dim=0)\n",
    "            GGU_P1_node_features = torch.cat((h, proxy_vertex), dim=0)\n",
    "            SGU_edge_features = torch.cat((e, torch.ones(h.shape[0],18).to(self.device)))\n",
    "            GGU_P2_edge_features = torch.ones(h.shape[0],18).to(self.device)\n",
    "            SGU_g.ndata['feat'] = SGU_node_features\n",
    "            GGU_P2_g.ndata['feat'] = GGU_P1_node_features\n",
    "            SGU_g.edata['feat'] = SGU_edge_features\n",
    "            GGU_P2_g.edata['feat'] = GGU_P2_edge_features\n",
    "            \n",
    "            #Update indexes\n",
    "            node_index += node_length\n",
    "            edge_index += edge_length\n",
    "            \n",
    "            #Append graphs into corresponding sets\n",
    "            SGU_graphs.append(SGU_g)\n",
    "            GGU_P2_graphs.append(GGU_P2_g)\n",
    "            \n",
    "            if ind == 0:\n",
    "                current_ind = node_length\n",
    "            else:\n",
    "                current_ind = current_ind + node_length + 1\n",
    "            proxy_inds.append(current_ind)\n",
    "            \n",
    "        #Combine single graphs to form a new DGL graph\n",
    "        SGU_graphs = dgl.batch(SGU_graphs).to(self.device)\n",
    "        GGU_P2_graphs = dgl.batch(GGU_P2_graphs).to(self.device)\n",
    "        \n",
    "        #Obtain edge features of SGU and GGU part 2\n",
    "        SGU_k = SGU_graphs.edata['feat']\n",
    "        GGU_P2_k = GGU_P2_graphs.edata['feat']\n",
    "        \n",
    "        #graph construction in DGL for GGU part 1\n",
    "        num_global_neighbors = self.num_neighbors\n",
    "        proxy_inds = torch.tensor(proxy_inds)\n",
    "        target_inds = self.global_neighbors(proxy_vertices, num_global_neighbors)\n",
    "        for ind, i in enumerate(proxy_inds):\n",
    "            src1 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            tgt1 = proxy_inds[target_inds[ind,:]].to(self.device)\n",
    "            src2 = proxy_inds[target_inds[ind,:]].to(self.device)\n",
    "            tgt2 = torch.full([num_global_neighbors,], i).to(self.device)\n",
    "            \n",
    "            src = torch.cat((src1, src2))\n",
    "            tgt = torch.cat((tgt1, tgt2))\n",
    "            \n",
    "            global_src = torch.cat((global_src, src))\n",
    "            global_tgt = torch.cat((global_tgt, tgt)) \n",
    "        \n",
    "        #Ensure the number of vertex matches with original graph\n",
    "        last_src = torch.tensor([SGU_graphs.num_nodes()-1]).to(self.device)\n",
    "        last_tgt = torch.tensor([SGU_graphs.num_nodes()-1]).to(self.device)\n",
    "        global_src = torch.cat((global_src, last_src))\n",
    "        global_tgt = torch.cat((global_tgt, last_tgt))\n",
    "          \n",
    "        #Form new DGL graph for GGU part 1\n",
    "        GGU_P1_graphs = dgl.graph((global_src.long(), global_tgt.long())).to(self.device)\n",
    "        GGU_P1_k = torch.ones(proxy_inds.shape[0]*num_global_neighbors*2+1, 18).to(self.device)\n",
    "\n",
    "        return SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, SGU_k, GGU_P1_k, GGU_P2_k\n",
    "    \n",
    "    def forward(self, graphs):\n",
    "        indices = []    \n",
    "        edge_lengths = []\n",
    "        node_lengths = []\n",
    "        for graph in dgl.unbatch(graphs):\n",
    "            adj = graph.adjacency_matrix(transpose=False)._indices()\n",
    "            ind1 = adj[0]\n",
    "            ind2 = adj[1]\n",
    "            inds = [ind1, ind2]\n",
    "            edge_lengths.append(graph.num_edges())\n",
    "            node_lengths.append(graph.num_nodes())\n",
    "            indices.append(inds)\n",
    "        \n",
    "        #GIG data\n",
    "        SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, SGU_k, GGU_P1_k, GGU_P2_k = \\\n",
    "        self.GDG(graphs, indices, edge_lengths, node_lengths)\n",
    "        h = SGU_graphs.ndata['feat']\n",
    "        \n",
    "        #Forward propagation\n",
    "        for ind, conv in enumerate(self.layers):\n",
    "            if ind < len(self.layers)-1:\n",
    "                SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, h, SGU_k, GGU_P1_k, GGU_P2_k = \\\n",
    "                conv(SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, h, SGU_k, GGU_P1_k, GGU_P2_k, ind)\n",
    "            else:\n",
    "                best_scores = \\\n",
    "                conv(SGU_graphs, GGU_P1_graphs, GGU_P2_graphs, h, SGU_k, GGU_P1_k, GGU_P2_k, ind)\n",
    "        return self.MLP_layer(best_scores)\n",
    "\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "        loss = criterion(pred, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "net_params = {}\n",
    "net_params['device'] = device\n",
    "net_params['gated'] = True \n",
    "net_params['in_dim'] = dataset.all.graph_lists[0].ndata['feat'][0].shape[0]\n",
    "net_params['in_dim_edge'] = dataset.all.graph_lists[0].edata['feat'][0].shape[0]\n",
    "net_params['residual'] = True\n",
    "net_params['hidden_dim'] = 100 \n",
    "net_params['out_dim'] = 100\n",
    "num_classes = len(np.unique(dataset.all.graph_labels))\n",
    "net_params['n_classes'] = num_classes\n",
    "net_params['n_heads'] = -1\n",
    "net_params['L'] = 4  \n",
    "net_params['readout'] = 'mean'\n",
    "net_params['layer_norm'] = True\n",
    "net_params['batch_norm'] = True\n",
    "net_params['in_feat_dropout'] = 0.0\n",
    "net_params['dropout'] = 0.1\n",
    "net_params['edge_feat'] = True\n",
    "net_params['self_loop'] = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "torch.cuda.manual_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(model, device, data_loader, epoch):\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_acc = 0\n",
    "    nb_data = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "            batch_graphs = batch_graphs.to(device)\n",
    "            batch_x = batch_graphs.ndata['feat'].to(device)\n",
    "            batch_e = batch_graphs.edata['feat'].to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            batch_scores = model.forward(batch_graphs)\n",
    "            loss = model.loss(batch_scores, batch_labels) \n",
    "            epoch_test_loss += loss.detach().item()\n",
    "            epoch_test_acc += accuracy(batch_scores, batch_labels)\n",
    "            nb_data += batch_labels.size(0)\n",
    "        epoch_test_loss /= (iter + 1)\n",
    "        epoch_test_acc /= nb_data\n",
    "    return epoch_test_loss, epoch_test_acc\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, device, data_loader, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_train_acc = 0\n",
    "    nb_data = 0\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_graphs = batch_graphs.to(device)\n",
    "        batch_x = batch_graphs.ndata['feat'].to(device)\n",
    "        batch_e = batch_graphs.edata['feat'].to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_scores = model.forward(batch_graphs)\n",
    "        loss = model.loss(batch_scores, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_train_acc += accuracy(batch_scores, batch_labels)\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_train_acc /= nb_data\n",
    "    return epoch_loss, epoch_train_acc, optimizer\n",
    "\n",
    "\n",
    "def accuracy(scores, targets):\n",
    "    scores = scores.detach().argmax(dim=1)\n",
    "    acc = (scores==targets).float().sum().item()\n",
    "    return acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_acc = []\n",
    "avg_train_acc = []\n",
    "avg_convergence_epochs = []\n",
    "\n",
    "t0 = time.time()\n",
    "per_epoch_time = []\n",
    "\n",
    "dataset = LoadData(\"ENZYMES\")\n",
    "\n",
    "\n",
    "\n",
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test\n",
    "\n",
    "root_log_dir = os.path.join('/root/tf-logs/', \"DATA_ENZYMES\")\n",
    "model_store_dir = os.path.join('/root/models/', \"ENZYMES_MODEL\")\n",
    "device = net_params['device']\n",
    "final_best = 0\n",
    "\n",
    "try:\n",
    "    for split_number in range(10):\n",
    "        t0_split = time.time()\n",
    "        log_dir = os.path.join(root_log_dir, \"RUN_69\" + str(split_number))\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        # setting seeds\n",
    "        np.random.seed(41)\n",
    "        torch.manual_seed(41)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.manual_seed(41)\n",
    "\n",
    "        print(\"RUN NUMBER: \", split_number)\n",
    "        trainset, valset, testset = dataset.train[split_number], dataset.val[split_number], dataset.test[split_number]\n",
    "        print(\"Training Graphs: \", len(trainset))\n",
    "        print(\"Validation Graphs: \", len(valset))\n",
    "        print(\"Test Graphs: \", len(testset))\n",
    "        print(\"Number of Classes: \", net_params['n_classes'])\n",
    "\n",
    "        model = GIGNet(net_params, 2, 14, device)\n",
    "        model = model.to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0012, weight_decay=0.14)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                         factor=0.5,\n",
    "                                                         patience=20,\n",
    "                                                         verbose=True)\n",
    "        best_split_model = 0\n",
    "        best_split_score = 0\n",
    "        epoch_train_losses, epoch_val_losses = [], []\n",
    "        epoch_train_accs, epoch_val_accs = [], [] \n",
    "\n",
    "\n",
    "        train_loader = DataLoader(trainset, batch_size=16, shuffle=True, drop_last=True, collate_fn=dataset.collate)\n",
    "        val_loader = DataLoader(valset, batch_size=16, shuffle=False, drop_last=True, collate_fn=dataset.collate)\n",
    "        test_loader = DataLoader(testset, batch_size=16, shuffle=False, drop_last=True, collate_fn=dataset.collate)\n",
    "\n",
    "        with tqdm(range(250)) as t:\n",
    "            for epoch in t:\n",
    "\n",
    "                t.set_description('Epoch %d' % epoch)    \n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                \n",
    "                epoch_train_loss, epoch_train_acc, optimizer = train_epoch(model, optimizer, device, train_loader, epoch)\n",
    "\n",
    "                epoch_val_loss, epoch_val_acc = evaluate_network(model, device, val_loader, epoch)\n",
    "                _, epoch_test_acc = evaluate_network(model, device, test_loader, epoch)\n",
    "\n",
    "                epoch_train_losses.append(epoch_train_loss)\n",
    "                epoch_val_losses.append(epoch_val_loss)\n",
    "                epoch_train_accs.append(epoch_train_acc)\n",
    "                epoch_val_accs.append(epoch_val_acc)\n",
    "\n",
    "                writer.add_scalar('train/_loss', epoch_train_loss, epoch)\n",
    "                writer.add_scalar('val/_loss', epoch_val_loss, epoch)\n",
    "                writer.add_scalar('train/_acc', epoch_train_acc, epoch)\n",
    "                writer.add_scalar('val/_acc', epoch_val_acc, epoch)\n",
    "                writer.add_scalar('test/_acc', epoch_test_acc, epoch)\n",
    "                writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "                _, epoch_test_acc = evaluate_network(model, device, test_loader, epoch)\n",
    "                \n",
    "                if epoch_test_acc > best_split_score:\n",
    "                    best_split_score = epoch_test_acc\n",
    "                    best_split_model = copy.deepcopy(model)\n",
    "                    \n",
    "                    \n",
    "                t.set_postfix(time=time.time()-start, lr=optimizer.param_groups[0]['lr'],\n",
    "                              train_loss=epoch_train_loss, val_loss=epoch_val_loss,\n",
    "                              train_acc=epoch_train_acc, val_acc=epoch_val_acc,\n",
    "                              test_acc=epoch_test_acc)  \n",
    "\n",
    "                per_epoch_time.append(time.time()-start)\n",
    "\n",
    "\n",
    "                scheduler.step(epoch_val_loss)\n",
    "\n",
    "                if optimizer.param_groups[0]['lr'] < 0.0:\n",
    "                    print(\"\\n!! LR EQUAL TO MIN LR SET.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        _, test_acc = evaluate_network(best_split_model, device, test_loader, epoch)   \n",
    "        _, train_acc = evaluate_network(best_split_model, device, train_loader, epoch)    \n",
    "        avg_test_acc.append(test_acc)   \n",
    "        avg_train_acc.append(train_acc)\n",
    "        avg_convergence_epochs.append(epoch)\n",
    "        if test_acc > final_best:\n",
    "            final_best = test_acc\n",
    "#             torch.save(model.state_dict(), '{}.pkl'.format(model_store_dir + \"/epoch_\" + str(epoch)))\n",
    "\n",
    "        print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
    "        print(\"Train Accuracy: {:.4f}\".format(train_acc))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early because of KeyboardInterrupt')\n",
    "\n",
    "print(\"TEST ACCURACY [FINAL RESULT] {:.4f}\".format(final_best))\n",
    "print(\"TOTAL TIME TAKEN: {:.4f}hrs\".format((time.time()-t0)/3600))\n",
    "print(\"AVG TIME PER EPOCH: {:.4f}s\".format(np.mean(per_epoch_time)))\n",
    "print(\"AVG CONVERGENCE Time (Epochs): {:.4f}\".format(np.mean(np.array(avg_convergence_epochs))))\n",
    "# Final test accuracy value averaged over 10-fold\n",
    "print(\"\"\"\\n\\n\\nFINAL RESULTS\\n\\nTEST ACCURACY averaged: {:.4f} with s.d. {:.4f}\"\"\"          .format(np.mean(np.array(avg_test_acc))*100, np.std(avg_test_acc)*100))\n",
    "print(\"\\nAll splits Test Accuracies:\\n\", avg_test_acc)\n",
    "print(\"\"\"\\n\\n\\nFINAL RESULTS\\n\\nTRAIN ACCURACY averaged: {:.4f} with s.d. {:.4f}\"\"\"          .format(np.mean(np.array(avg_train_acc))*100, np.std(avg_train_acc)*100))\n",
    "print(\"\\nAll splits Train Accuracies:\\n\", avg_train_acc)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
